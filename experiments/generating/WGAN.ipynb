{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(\"../\")\n",
    "from utils.evaluation import evaluate\n",
    "from utils.metrics import Metrics\n",
    "from models.NNGenerator import AdaptableDenseModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import neptune\n",
    "from neptune_pytorch import NeptuneLogger\n",
    "from neptune.utils import stringify_unsupported\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from git import Repo\n",
    "\n",
    "# Get the git root directory\n",
    "repo = Repo(\".\", search_parent_directories=True)\n",
    "git_root = repo.git.rev_parse(\"--show-toplevel\")\n",
    "\n",
    "# Load data\n",
    "X_Train_pd = pickle.load(open(f\"{git_root}/data/splits/train/X_pandas.pck\", \"rb\"))\n",
    "y_Train_pd = pickle.load(open(f\"{git_root}/data/splits/train/y_pandas.pck\", \"rb\"))\n",
    "\n",
    "X_Val_pd = pickle.load(open(f\"{git_root}/data/splits/val/X_pandas.pck\", \"rb\"))\n",
    "y_Val_pd = pickle.load(open(f\"{git_root}/data/splits/val/y_pandas.pck\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train = torch.tensor(X_Train_pd.values, dtype=torch.float32)\n",
    "y_Train = torch.tensor(y_Train_pd.values, dtype=torch.float32)\n",
    "\n",
    "X_Val = torch.tensor(X_Val_pd.values, dtype=torch.float32)\n",
    "y_Val = torch.tensor(y_Val_pd.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_Train, y_Train)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_Val, y_Val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_from_logits(y_hat: torch.Tensor, threshold = 0.5) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        y_pred_tensor = (torch.sigmoid(y_hat) > threshold).float()\n",
    "    return y_pred_tensor\n",
    "\n",
    "\n",
    "def evaluate_from_dataframe(X: pd.DataFrame):\n",
    "    X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32)\n",
    "    \n",
    "    #model: a pytorch model, which transforms X -> y in torch.Tensor format\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    y_pred_tensor = label_from_logits(model(X_tensor))\n",
    "    \n",
    "    return pd.DataFrame(y_pred_tensor.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.aats import compute_AAts\n",
    "#from metrics.precision_recall import get_precision_recall\n",
    "\n",
    "def calc_aat(real_data, fake_data):\n",
    "    _, _, aat = compute_AAts(real_data, fake_data)\n",
    "    return aat\n",
    "\n",
    "# def calc_precision_recall(real_data, fake_data):\n",
    "#     precision, recall = get_precision_recall(real_data, fake_data)\n",
    "#     return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Adapted from https://forge.ibisc.univ-evry.fr/alacan/GANs-for-transcriptomics/-/blob/master/src/models/utils.py?ref_type=heads\n",
    "def wasserstein_loss(y_true: torch.tensor, y_pred: torch.tensor):\n",
    "    \"\"\"\n",
    "    Returns Wasserstein loss (product of real/fake labels and critic scores on real or fake data)\n",
    "    ----\n",
    "    Parameters:\n",
    "        y_true (torch.tensor): true labels (either real or fake)\n",
    "        y_pred (torch.tensor): critic scores on real or fake data\n",
    "    Returns:\n",
    "        (torch.tensor): mean product of real labels and critic scores\n",
    "    \"\"\"\n",
    "    return torch.mean(y_true * y_pred)\n",
    "\n",
    "\n",
    "def generator_loss(fake_score: torch.tensor):\n",
    "    \"\"\"\n",
    "    Returns generator loss i.e the negative scores of the critic on fake data.\n",
    "    ----\n",
    "    Parameters:\n",
    "        fake_score (torch.tensor): critic scores on fake data\n",
    "    Returns:\n",
    "        (torch.tensor): generator loss\"\"\"\n",
    "\n",
    "    return wasserstein_loss(-torch.ones_like(fake_score), fake_score)\n",
    "\n",
    "\n",
    "def discriminator_loss(real_score: torch.tensor, fake_score: torch.tensor):\n",
    "    \"\"\"\n",
    "    Compute and return the wasserstein loss of critic scores on real and fake data i.e: wassertstein_loss = mean(-score_real) + mean(score_fake)\n",
    "    ----\n",
    "    Parameters:\n",
    "        real_score (torch.tensor): critic scores on real data\n",
    "        fake_score (torch.tensor): critic scores on fake data\n",
    "    Returns:\n",
    "        (torch.tensor): wasserstein loss\n",
    "    \"\"\"\n",
    "    real_loss = wasserstein_loss(-torch.ones_like(real_score), real_score)\n",
    "    fake_loss = wasserstein_loss(torch.ones_like(fake_score), fake_score)\n",
    "\n",
    "    return real_loss, fake_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(D, G, d_optimizer, g_optimizer, train_loader, val_loader, epochs, latentSpaceSize, lambda_gp, iters_critic, device, neptune_logger=None, run = None, trial = None):\n",
    "# Code adopted from https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "# and https://github.com/Zeleni9/pytorch-wgan/blob/master/models/wgan_gradient_penalty.py\n",
    "    D = D.to(device)\n",
    "    G = G.to(device)\n",
    "    aat = 0\n",
    "\n",
    "    print(\"Starting Training Loop...\")\n",
    "    # For each epoch\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        errDItem = 0\n",
    "        errGItem = 0\n",
    "        Wasserstein_D = 0\n",
    "        G.train()\n",
    "        D.train()\n",
    "\n",
    "        # For each batch in the dataloader\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            y = y.to(device).long()\n",
    "            \n",
    "            ############################\n",
    "            # (1) Update D network: maximize D(x|c) - D(G(z|c)) + lambda_gp * gradient_penalty\n",
    "            ###########################\n",
    "            ## Train with all-real batch\n",
    "            for p in D.parameters():\n",
    "                p.requires_grad = True  \n",
    "\n",
    "            for p in G.parameters():\n",
    "                p.requires_grad = False  # to avoid computation\n",
    "\n",
    "            d_optimizer.zero_grad()\n",
    "            b_size = X.size(0)\n",
    "\n",
    "            real_data = X.to(device)\n",
    "\n",
    "            for crit_step in range(iters_critic):\n",
    "                # Forward pass real batch through D\n",
    "                \n",
    "                errD_real = D(real_data, y)\n",
    "\n",
    "                ## Train with all-fake batch\n",
    "                # Generate batch of latent vectors\n",
    "                z = torch.randn(b_size, latentSpaceSize, device=device)\n",
    "                # Generate fake image batch with G\n",
    "                fake_data = G(z, y)\n",
    "                # Classify all fake batch with D\n",
    "                errD_fake = D(fake_data.detach(), y)\n",
    "                # Calculate D's loss on the all-fake batch\n",
    "                real_loss, fake_loss = discriminator_loss(errD_real, errD_fake)\n",
    "\n",
    "                # Loss for D in\n",
    "                d_loss = real_loss + fake_loss\n",
    "                Wasserstein_D += d_loss.item()\n",
    "\n",
    "                # Fixed batch size\n",
    "                BATCH_SIZE = real_data.size(0)\n",
    "\n",
    "                # Sample alpha from uniform distribution\n",
    "                alpha = torch.rand(\n",
    "                    BATCH_SIZE,\n",
    "                    1,\n",
    "                    requires_grad=True,\n",
    "                    device=real_data.device)\n",
    "\n",
    "                # Interpolation between real data and fake data.\n",
    "                interpolation = torch.mul(alpha, real_data) + \\\n",
    "                    torch.mul((1 - alpha), fake_data)\n",
    "\n",
    "                # Get outputs from critic\n",
    "                disc_outputs = D(interpolation, y)\n",
    "                grad_outputs = torch.ones_like(\n",
    "                    disc_outputs,\n",
    "                    requires_grad=False,\n",
    "                    device=real_data.device)\n",
    "\n",
    "                # Retrieve gradients\n",
    "                gradients = torch.autograd.grad(\n",
    "                    outputs=disc_outputs,\n",
    "                    inputs=interpolation,\n",
    "                    grad_outputs=grad_outputs,\n",
    "                    create_graph=True,\n",
    "                    retain_graph=True)[0]\n",
    "\n",
    "                # Compute gradient penalty\n",
    "                gradients = gradients.view(BATCH_SIZE, -1)\n",
    "                grad_norm = gradients.norm(2, dim=1)\n",
    "\n",
    "                gradient_penalty = torch.mean((grad_norm - 1) ** 2) * lambda_gp\n",
    "                \n",
    "                d_loss += gradient_penalty\n",
    "                # Calculate gradients for D in backward pass\n",
    "                d_loss.backward()\n",
    "\n",
    "                # Compute error of D as sum of the errors on the real and fake batches and the gradient penalty\n",
    "                errDItem += d_loss.item()\n",
    "                # Update D\n",
    "                d_optimizer.step()\n",
    "\n",
    "            Wasserstein_D /= iters_critic\n",
    "            errDItem /= iters_critic\n",
    "            ############################\n",
    "            # (2) Update G network: maximize D(G(z|c))\n",
    "            ###########################\n",
    "            for p in D.parameters():\n",
    "                p.requires_grad = False  # to avoid computation\n",
    "\n",
    "            for p in G.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "            g_optimizer.zero_grad()\n",
    "\n",
    "            z = torch.randn(b_size, latentSpaceSize, device=device)\n",
    "            # Generate fake image batch with G\n",
    "            fake_data = G(z, y)\n",
    "            # Calculate G's loss based on this output\n",
    "            errG = D(fake_data, y)\n",
    "            \n",
    "            # Calculate gradients for G\n",
    "            g_loss = generator_loss(errG)\n",
    "            g_loss.backward()\n",
    "            errGItem += g_loss.item()\n",
    "            # Update G\n",
    "            g_optimizer.step()\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            with torch.no_grad():\n",
    "                G.eval()\n",
    "                real_datas = np.array([])\n",
    "                fake_datas = np.array([])\n",
    "                for X_val, y_val in val_loader:\n",
    "                    y_val = y_val.to(device).long()\n",
    "                    X_val = X_val.to(device).to(torch.float16)\n",
    "                    z = torch.randn(X_val.size(0), latentSpaceSize)\n",
    "                    fake_data = G(z.to(device), y_val).to(torch.float16)\n",
    "                    real_datas = torch.vstack((real_datas, X_val)) if real_datas.size else real_data\n",
    "                    fake_datas = torch.vstack((fake_datas, fake_data)) if fake_datas.size else fake_data\n",
    "                aat = calc_aat(real_datas.cpu(), fake_datas.cpu())\n",
    "            #with torch.device(device):\n",
    "            #    recall, precision = calc_precision_recall(real_datas, fake_datas)\n",
    "\n",
    "        errDItem /= len(train_loader)\n",
    "        errGItem /= len(train_loader)\n",
    "        Wasserstein_D /= len(train_loader)\n",
    "        if neptune_logger is not None:\n",
    "            run[neptune_logger.base_namespace]['D_Loss'].append(errDItem)\n",
    "            run[neptune_logger.base_namespace]['G_Loss'].append(errGItem)\n",
    "            run[neptune_logger.base_namespace]['Wasserstein_D'].append(Wasserstein_D)\n",
    "            run[neptune_logger.base_namespace]['AAT'].append(aat)\n",
    "            #run[neptune_logger.base_namespace]['Precision'].append(precision)\n",
    "            #run[neptune_logger.base_namespace]['Recall'].append(recall)\n",
    "\n",
    "        print(f\"Epoch {epoch} D Loss: {errDItem} G Loss: {errGItem} Wasserstein D: {Wasserstein_D}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, output_size, class_size, embedding_size):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024),\n",
    "            #nn.Linear(input_size + embedding_size * class_size, 512),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 400),\n",
    "            nn.BatchNorm1d(400),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(400, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, output_size),\n",
    "        )\n",
    "        self.embedding = nn.Embedding(class_size, embedding_size)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "    def forward(self, x, cls):\n",
    "        #cls = self.embedding(cls)\n",
    "        #x = torch.cat((x, cls.flatten(start_dim=1)), 1)\n",
    "        return self.model(x)\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, output_size, class_size, embedding_size):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            #nn.Linear(input_size + embedding_size * class_size, 512),\n",
    "            nn.Linear(input_size , 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(2048, output_size)\n",
    "        )\n",
    "        self.embedding = nn.Embedding(class_size, embedding_size)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "    def forward(self, x, cls):\n",
    "        #cls = self.embedding(cls)\n",
    "        #x = torch.cat((x, cls.flatten(start_dim=1)), 1)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim_data = X_Train.shape[1]\n",
    "output_dim_data = y_Train.shape[1]\n",
    "latentSpaceSize = 128\n",
    "\n",
    "# Define the generator\n",
    "\n",
    "G = Generator(latentSpaceSize, input_dim_data, output_dim_data, 2)\n",
    "\n",
    "# Define the discriminator\n",
    "D = Discriminator(input_dim_data, 16, output_dim_data, 2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "params = {\n",
    "    \"epochs\": 800,\n",
    "    \"lambda_gp\": 10,\n",
    "    \"latentSpaceSize\": latentSpaceSize,\n",
    "    \"device\": device,\n",
    "    \"batch_size\": 512,\n",
    "    \"lr_g\": 0.0001,\n",
    "    \"lr_d\": 0.0008,\n",
    "    \"critic_iter\": 6,\n",
    "    \"b1\": 0.5,\n",
    "    \"b2\": 0.999\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=params[\"lr_d\"], betas=(params[\"b1\"], params[\"b2\"]))\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=params[\"lr_g\"], betas=(params[\"b1\"], params[\"b2\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneWarning: By default, these monitoring options are disabled in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', 'capture_hardware_metrics'. You can set them to 'True' when initializing the run and the monitoring will continue until you call run.stop() or the kernel stops. NOTE: To track the source files, pass their paths to the 'source_code' argument. For help, see: https://docs.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/JPL/rna-sequencing/e/RNAS-183\n",
      "Starting Training Loop...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a417613e5640bbb3d5cadae12cc398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "100%|██████████| 4/4 [00:17<00:00,  4.42s/it]\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 D Loss: 0.035091672862384383 G Loss: -0.05762551410996414 Wasserstein D: 6.72552447685773e-05\n",
      "Epoch 1 D Loss: 0.04260598517985928 G Loss: -0.44061562898275736 Wasserstein D: -0.0002594679312296642\n",
      "Epoch 2 D Loss: 0.04062837096103206 G Loss: -0.3771143027327277 Wasserstein D: -0.00018668166625630432\n",
      "Epoch 3 D Loss: 0.024141028851231438 G Loss: -0.2404989467321576 Wasserstein D: -0.00021485700903980349\n",
      "Epoch 4 D Loss: 0.017893668101375205 G Loss: -0.31839795275167987 Wasserstein D: -0.0002252476338433242\n",
      "Epoch 5 D Loss: 0.009113774584697325 G Loss: -0.496205041041741 Wasserstein D: -0.0003227011369971801\n",
      "Epoch 6 D Loss: 0.00934959159122619 G Loss: -0.6267793524515378 Wasserstein D: -0.0008520398970363615\n",
      "Epoch 7 D Loss: 0.006029940568200679 G Loss: -0.758686542927802 Wasserstein D: -0.002049036654661386\n",
      "Epoch 8 D Loss: 0.0025165940816169663 G Loss: -0.8926777906351157 Wasserstein D: -0.004235582835052569\n",
      "Epoch 9 D Loss: 0.0011501017664687155 G Loss: -0.7369100380610752 Wasserstein D: -0.003230553716005974\n",
      "Epoch 10 D Loss: 0.00292987735527289 G Loss: -0.6328469854551595 Wasserstein D: -0.0021557599294823246\n",
      "Epoch 11 D Loss: -0.01082697833792456 G Loss: -0.39335031221926836 Wasserstein D: -0.01729694342434672\n",
      "Epoch 12 D Loss: -0.011036588059767835 G Loss: 0.36988378989581877 Wasserstein D: -0.018403666333302718\n",
      "Epoch 13 D Loss: -0.017548110279984534 G Loss: 0.9249387771099598 Wasserstein D: -0.026160624398217935\n",
      "Epoch 14 D Loss: -0.03391148091613586 G Loss: 1.815606451117909 Wasserstein D: -0.04659376018911989\n",
      "Epoch 15 D Loss: -0.05290085545054653 G Loss: 2.169104508534595 Wasserstein D: -0.0641560618882641\n",
      "Epoch 16 D Loss: 0.11543875862076616 G Loss: 2.0341320106616387 Wasserstein D: -0.07068130001626151\n",
      "Epoch 17 D Loss: -0.09172665039401083 G Loss: 3.5937820297854763 Wasserstein D: -0.1325669841706665\n",
      "Epoch 18 D Loss: -0.1412955388038706 G Loss: 3.683025782966947 Wasserstein D: -0.1747429054689001\n",
      "Epoch 19 D Loss: -0.135474041204998 G Loss: 5.762589161212627 Wasserstein D: -0.18549773922734644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:18<00:00,  4.56s/it]\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 D Loss: -0.2785913818757723 G Loss: 9.752666086583705 Wasserstein D: -0.31950820943194114\n",
      "Epoch 21 D Loss: -0.36850917222694857 G Loss: 7.561513130481426 Wasserstein D: -0.46777252396729957\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m\n\u001b[0;32m     10\u001b[0m run[neptune_logger\u001b[38;5;241m.\u001b[39mbase_namespace][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG_Structure\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(G)\n\u001b[0;32m     11\u001b[0m run[neptune_logger\u001b[38;5;241m.\u001b[39mbase_namespace][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD_Structure\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(D)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatentSpaceSize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlambda_gp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcritic_iter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneptune_logger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneptune_logger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m neptune_logger\u001b[38;5;241m.\u001b[39mlog_model()\n\u001b[0;32m     18\u001b[0m run\u001b[38;5;241m.\u001b[39mstop()\n",
      "Cell \u001b[1;32mIn[8], line 96\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(D, G, d_optimizer, g_optimizer, train_loader, val_loader, epochs, latentSpaceSize, lambda_gp, iters_critic, device, neptune_logger, run, trial)\u001b[0m\n\u001b[0;32m     93\u001b[0m d_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Compute error of D as sum of the errors on the real and fake batches and the gradient penalty\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m errDItem \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43md_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Update D\u001b[39;00m\n\u001b[0;32m     98\u001b[0m d_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run = neptune.init_run(\n",
    "    api_token=os.getenv(\"NEPTUNE_API_KEY\"),\n",
    "    project=os.getenv(\"NEPTUNE_PROJECT_NAME\"),\n",
    "    name=\"WGAN-GP - 800 - simple\",\n",
    ")\n",
    "\n",
    "neptune_logger = NeptuneLogger(run=run, model=G)\n",
    "                               \n",
    "run[neptune_logger.base_namespace][\"hyperparams\"] = stringify_unsupported(params)\n",
    "run[neptune_logger.base_namespace][\"G_Structure\"] = str(G)\n",
    "run[neptune_logger.base_namespace][\"D_Structure\"] = str(D)\n",
    "\n",
    "\n",
    "\n",
    "training(D, G, d_optimizer, g_optimizer, train_loader , val_loader, params[\"epochs\"], latentSpaceSize, params[\"lambda_gp\"], params[\"critic_iter\"], device, neptune_logger=neptune_logger, run=run)\n",
    "\n",
    "neptune_logger.log_model()\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "#torch.save(G.state_dict(), f\"{git_root}/experiments/generating/models/G_800_WGAN_GP.pt\")\n",
    "#torch.save(D.state_dict(), f\"{git_root}/experiments/generating/models/D_800_WGAN_GP.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.1)\n",
       "    (3): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.1)\n",
       "    (6): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "    (7): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): LeakyReLU(negative_slope=0.1)\n",
       "    (9): Linear(in_features=2048, out_features=5045, bias=True)\n",
       "  )\n",
       "  (embedding): Embedding(105, 2)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3123,  0.3945, -0.2576,  ...,  3.7707,  0.0848, -0.4399]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G(torch.randn(1, latentSpaceSize, device=device), y_Val[0].long().unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000,  ..., 3.3789, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(116918.2500)\n"
     ]
    }
   ],
   "source": [
    "# Classify the discriminator on the first 50 validation data points\n",
    "with torch.no_grad():\n",
    "    D.eval().cpu()\n",
    "    y_pred = D(X_Val[:50], y_Val[:50].long())\n",
    "    print(y_pred.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1082.1344,   354.4251,   904.1985,  ...,  -389.1379,   386.4493,\n",
      "         -1525.4122],\n",
      "        [  319.9232,  1458.5280,    78.4604,  ...,  -986.5058,  1140.0000,\n",
      "           306.9229],\n",
      "        [ 1623.1976,  1710.8981,  1674.1731,  ...,   239.8209,  2076.3311,\n",
      "           552.0113],\n",
      "        ...,\n",
      "        [ 1203.5183,   879.1621,   824.4224,  ..., -1108.3838,   656.3746,\n",
      "         -1935.3901],\n",
      "        [  -81.6840,   884.2855,  -201.2603,  ...,  -539.1361,   656.3450,\n",
      "           598.7426],\n",
      "        [  528.3951,  1721.8861,   215.2771,  ..., -1230.1609,  1348.0797,\n",
      "            78.7755]])\n",
      "tensor(96606.2500)\n"
     ]
    }
   ],
   "source": [
    "# Generate 50 samples\n",
    "with torch.no_grad():\n",
    "    G.eval().cpu()\n",
    "    fake_data = G(torch.randn(50, latentSpaceSize), y_Val[:50].long())\n",
    "    print(fake_data)\n",
    "\n",
    "# Evaluate the generated data on the discriminator\n",
    "with torch.no_grad():\n",
    "    D.eval().cpu()\n",
    "    y_pred = D(fake_data, y_Val[:50].long())\n",
    "    print(y_pred.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
