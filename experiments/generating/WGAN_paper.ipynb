{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(\"../\")\n",
    "from utils.evaluation import evaluate\n",
    "from utils.metrics import Metrics\n",
    "from models.NNGenerator import AdaptableDenseModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import neptune\n",
    "from neptune_pytorch import NeptuneLogger\n",
    "from neptune.utils import stringify_unsupported\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from git import Repo\n",
    "\n",
    "# Get the git root directory\n",
    "repo = Repo(\".\", search_parent_directories=True)\n",
    "git_root = repo.git.rev_parse(\"--show-toplevel\")\n",
    "\n",
    "# Load data\n",
    "X_Train_pd = pickle.load(open(f\"{git_root}/data/splits/train/X_pandas.pck\", \"rb\"))\n",
    "y_Train_pd = pickle.load(open(f\"{git_root}/data/splits/train/y_pandas.pck\", \"rb\"))\n",
    "\n",
    "X_Val_pd = pickle.load(open(f\"{git_root}/data/splits/val/X_pandas.pck\", \"rb\"))\n",
    "y_Val_pd = pickle.load(open(f\"{git_root}/data/splits/val/y_pandas.pck\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train = torch.tensor(X_Train_pd.values, dtype=torch.float32)\n",
    "y_Train = torch.tensor(y_Train_pd.values, dtype=torch.float32)\n",
    "\n",
    "X_Val = torch.tensor(X_Val_pd.values, dtype=torch.float32)\n",
    "y_Val = torch.tensor(y_Val_pd.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_Train, y_Train)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_Val, y_Val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_from_logits(y_hat: torch.Tensor, threshold = 0.5) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        y_pred_tensor = (torch.sigmoid(y_hat) > threshold).float()\n",
    "    return y_pred_tensor\n",
    "\n",
    "\n",
    "def evaluate_from_dataframe(X: pd.DataFrame):\n",
    "    X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32)\n",
    "    \n",
    "    #model: a pytorch model, which transforms X -> y in torch.Tensor format\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    y_pred_tensor = label_from_logits(model(X_tensor))\n",
    "    \n",
    "    return pd.DataFrame(y_pred_tensor.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adopted from https://forge.ibisc.univ-evry.fr/alacan/GANs-for-transcriptomics/-/blob/master/src/models/utils.py?ref_type=heads\n",
    "\n",
    "from metrics.aats import compute_AAts\n",
    "#from metrics.precision_recall import get_precision_recall\n",
    "\n",
    "def calc_aat(real_data, fake_data):\n",
    "    _, _, aat = compute_AAts(real_data, fake_data)\n",
    "    return aat\n",
    "\n",
    "# def calc_precision_recall(real_data, fake_data):\n",
    "#     precision, recall = get_precision_recall(real_data, fake_data)\n",
    "#     return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Adopted from https://forge.ibisc.univ-evry.fr/alacan/GANs-for-transcriptomics/-/blob/master/src/models/utils.py?ref_type=heads\n",
    "def wasserstein_loss(y_true: torch.tensor, y_pred: torch.tensor):\n",
    "    \"\"\"\n",
    "    Returns Wasserstein loss (product of real/fake labels and critic scores on real or fake data)\n",
    "    ----\n",
    "    Parameters:\n",
    "        y_true (torch.tensor): true labels (either real or fake)\n",
    "        y_pred (torch.tensor): critic scores on real or fake data\n",
    "    Returns:\n",
    "        (torch.tensor): mean product of real labels and critic scores\n",
    "    \"\"\"\n",
    "    return torch.mean(y_true * y_pred)\n",
    "\n",
    "\n",
    "def generator_loss(fake_score: torch.tensor):\n",
    "    \"\"\"\n",
    "    Returns generator loss i.e the negative scores of the critic on fake data.\n",
    "    ----\n",
    "    Parameters:\n",
    "        fake_score (torch.tensor): critic scores on fake data\n",
    "    Returns:\n",
    "        (torch.tensor): generator loss\"\"\"\n",
    "\n",
    "    return wasserstein_loss(-torch.ones_like(fake_score), fake_score)\n",
    "\n",
    "\n",
    "def discriminator_loss(real_score: torch.tensor, fake_score: torch.tensor):\n",
    "    \"\"\"\n",
    "    Compute and return the wasserstein loss of critic scores on real and fake data i.e: wassertstein_loss = mean(-score_real) + mean(score_fake)\n",
    "    ----\n",
    "    Parameters:\n",
    "        real_score (torch.tensor): critic scores on real data\n",
    "        fake_score (torch.tensor): critic scores on fake data\n",
    "    Returns:\n",
    "        (torch.tensor): wasserstein loss\n",
    "    \"\"\"\n",
    "    real_loss = wasserstein_loss(-torch.ones_like(real_score), real_score)\n",
    "    fake_loss = wasserstein_loss(torch.ones_like(fake_score), fake_score)\n",
    "\n",
    "    return real_loss, fake_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(D, G, d_optimizer, g_optimizer, train_loader, val_loader, epochs, latentSpaceSize, lambda_gp, iters_critic, device, neptune_logger=None, run = None, trial = None):\n",
    "# Code adopted from https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "# and https://github.com/Zeleni9/pytorch-wgan/blob/master/models/wgan_gradient_penalty.py\n",
    "# and https://forge.ibisc.univ-evry.fr/alacan/GANs-for-transcriptomics/-/blob/master/src/models/utils.py?ref_type=heads\n",
    "    D = D.to(device)\n",
    "    G = G.to(device)\n",
    "    aat = 0\n",
    "\n",
    "    print(\"Starting Training Loop...\")\n",
    "    # For each epoch\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        errDItem = 0\n",
    "        errGItem = 0\n",
    "        Wasserstein_D = 0\n",
    "        G.train()\n",
    "        D.train()\n",
    "\n",
    "        # For each batch in the dataloader\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            y = y.to(device).long()\n",
    "            \n",
    "            ############################\n",
    "            # (1) Update D network: maximize D(x|c) - D(G(z|c)) + lambda_gp * gradient_penalty\n",
    "            ###########################\n",
    "            for p in D.parameters():\n",
    "                p.requires_grad = True  \n",
    "\n",
    "            for p in G.parameters():\n",
    "                p.requires_grad = False  # to avoid computation\n",
    "\n",
    "            # Train with all-real batch\n",
    "            b_size = X.size(0)\n",
    "            real_data = X.to(device)\n",
    "\n",
    "            # Train D\n",
    "            for crit_step in range(iters_critic):\n",
    "                d_optimizer.zero_grad()\n",
    "                \n",
    "                # Generate batch of latent vectors\n",
    "                z = torch.randn(b_size, latentSpaceSize, device=device)\n",
    "                # Generate fake image batch with G\n",
    "                fake_data = G(z, y)\n",
    "                NB_GENES = fake_data.size(1)\n",
    "\n",
    "                # Perform random augmentations for stability\n",
    "                augmentations = torch.distributions.binomial.Binomial(total_count=1, probs=0).sample(torch.tensor([b_size])).to(device)\n",
    "                fake_data = fake_data + augmentations[:, None] * torch.normal(0, 0.5, size=(NB_GENES,), device=device)\n",
    "                real_data = real_data + augmentations[:,None] * torch.normal(0, 0.5, size=(b_size,NB_GENES), device=device)\n",
    "\n",
    "                # Classify batches with D\n",
    "                errD_real = D(real_data, y)\n",
    "                errD_fake = D(fake_data, y)\n",
    "                # Calculate D's loss on the all-fake batch\n",
    "                real_loss, fake_loss = discriminator_loss(errD_real, errD_fake)\n",
    "\n",
    "                # Loss for D in\n",
    "                d_loss = real_loss + fake_loss\n",
    "                Wasserstein_D = d_loss.item()\n",
    "\n",
    "                ####### Gradient penalty #######\n",
    "                BATCH_SIZE = real_data.size(0)\n",
    "\n",
    "                # Sample alpha from uniform distribution\n",
    "                alpha = torch.rand(\n",
    "                    BATCH_SIZE,\n",
    "                    1,\n",
    "                    requires_grad=True,\n",
    "                    device=real_data.device)\n",
    "\n",
    "                # Interpolation between real data and fake data.\n",
    "                interpolation = torch.mul(alpha, real_data) + \\\n",
    "                    torch.mul((1 - alpha), fake_data)\n",
    "\n",
    "                # Get outputs from critic\n",
    "                disc_outputs = D(interpolation, y)\n",
    "                grad_outputs = torch.ones_like(\n",
    "                    disc_outputs,\n",
    "                    requires_grad=False,\n",
    "                    device=real_data.device)\n",
    "\n",
    "                # Retrieve gradients\n",
    "                gradients = torch.autograd.grad(\n",
    "                    outputs=disc_outputs,\n",
    "                    inputs=interpolation,\n",
    "                    grad_outputs=grad_outputs,\n",
    "                    create_graph=True,\n",
    "                    retain_graph=True)[0]\n",
    "\n",
    "                # Compute gradient penalty\n",
    "                gradients = gradients.view(BATCH_SIZE, -1)\n",
    "                grad_norm = gradients.norm(2, dim=1)\n",
    "\n",
    "                gradient_penalty = torch.mean((grad_norm - 1) ** 2) * lambda_gp\n",
    "                d_loss += gradient_penalty\n",
    "                # Calculate gradients for D in backward pass\n",
    "                d_loss.backward()\n",
    "\n",
    "                # Compute error of D as sum of the errors on the real and fake batches and the gradient penalty\n",
    "                errDItem = d_loss.item()\n",
    "                # Update D\n",
    "                d_optimizer.step()\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network: maximize D(G(z|c))\n",
    "            ###########################\n",
    "            for p in D.parameters():\n",
    "                p.requires_grad = False  # to avoid computation\n",
    "\n",
    "            for p in G.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            # Generate batch of latent vectors\n",
    "            z = torch.randn(b_size, latentSpaceSize, device=device)\n",
    "            # Generate fake image batch with G\n",
    "            fake_data = G(z, y)\n",
    "\n",
    "            NB_GENES = fake_data.shape[1]\n",
    "\n",
    "            # Perform random augmentations for stability\n",
    "            augmentations = torch.distributions.binomial.Binomial(total_count=1, probs=0).sample(torch.tensor([BATCH_SIZE])).to(device)\n",
    "            fake_data = fake_data + augmentations[:, None] * torch.normal(0, 0.5, size=(NB_GENES,), device=device)\n",
    "\n",
    "            # Calculate G's loss based on this output\n",
    "            errG = D(fake_data, y)\n",
    "            \n",
    "            # Calculate gradients for G\n",
    "            g_loss =  generator_loss(errG)\n",
    "            g_loss.backward()\n",
    "            errGItem += g_loss.item()\n",
    "            # Update G\n",
    "            g_optimizer.step()\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        if epoch % 20 == 0:\n",
    "            with torch.no_grad():\n",
    "                G.eval()\n",
    "                real_datas = np.array([])\n",
    "                fake_datas = np.array([])\n",
    "                for X_val, y_val in val_loader:\n",
    "                    y_val = y_val.to(device).long()\n",
    "                    X_val = X_val.to(device).to(torch.float16)\n",
    "                    z = torch.randn(X_val.size(0), latentSpaceSize)\n",
    "                    fake_data = G(z.to(device), y_val).to(torch.float16)\n",
    "                    real_datas = torch.vstack((real_datas, X_val)) if real_datas.size else real_data\n",
    "                    fake_datas = torch.vstack((fake_datas, fake_data)) if fake_datas.size else fake_data\n",
    "                aat = calc_aat(real_datas.cpu(), fake_datas.cpu())\n",
    "            #with torch.device(device):\n",
    "            #    recall, precision = calc_precision_recall(real_datas, fake_datas)\n",
    "\n",
    "        errDItem /= len(train_loader)\n",
    "        errGItem /= len(train_loader)\n",
    "        Wasserstein_D /= len(train_loader)\n",
    "        # Log the metrics to neptune\n",
    "        if neptune_logger is not None:\n",
    "            run[neptune_logger.base_namespace]['D_Loss'].append(errDItem)\n",
    "            run[neptune_logger.base_namespace]['G_Loss'].append(errGItem)\n",
    "            run[neptune_logger.base_namespace]['Wasserstein_D'].append(Wasserstein_D)\n",
    "            run[neptune_logger.base_namespace]['AAT'].append(aat)\n",
    "            #run[neptune_logger.base_namespace]['Precision'].append(precision)\n",
    "            #run[neptune_logger.base_namespace]['Recall'].append(recall)\n",
    "\n",
    "        print(f\"Epoch {epoch} D Loss: {errDItem} G Loss: {errGItem} Wasserstein D: {Wasserstein_D}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, output_size, class_size, embedding_size):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            #nn.Linear(input_size, 1024),\n",
    "            nn.Linear(input_size + embedding_size * class_size, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, output_size),\n",
    "        )\n",
    "        self.embedding = nn.Embedding(class_size, embedding_size)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "    def forward(self, x, cls):\n",
    "        cls = self.embedding(cls)\n",
    "        x = torch.cat((x, cls.flatten(start_dim=1)), 1)\n",
    "        return self.model(x)\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, output_size, class_size, embedding_size):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size + embedding_size * class_size, 512),\n",
    "            #nn.Linear(input_size , 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(2048, output_size)\n",
    "        )\n",
    "        self.embedding = nn.Embedding(class_size, embedding_size)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "    def forward(self, x, cls):\n",
    "        cls = self.embedding(cls)\n",
    "        x = torch.cat((x, cls.flatten(start_dim=1)), 1)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim_data = X_Train.shape[1]\n",
    "output_dim_data = y_Train.shape[1]\n",
    "latentSpaceSize = 128\n",
    "\n",
    "# Define the generator\n",
    "G = Generator(latentSpaceSize, input_dim_data, output_dim_data, 2)\n",
    "\n",
    "# Define the discriminator\n",
    "D = Discriminator(input_dim_data, 16, output_dim_data, 2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "params = {\n",
    "    \"epochs\": 800,\n",
    "    \"lambda_gp\": 10,\n",
    "    \"latentSpaceSize\": latentSpaceSize,\n",
    "    \"device\": device,\n",
    "    \"batch_size\": 512,\n",
    "    \"lr_g\": 0.0001/20,\n",
    "    \"lr_d\": 0.001/20,\n",
    "    \"critic_iter\": 5,\n",
    "    \"b1\": 0.5,\n",
    "    \"b2\": 0.999\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=params[\"lr_d\"], betas=(params[\"b1\"], params[\"b2\"]))\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=params[\"lr_g\"], betas=(params[\"b1\"], params[\"b2\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneWarning: By default, these monitoring options are disabled in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', 'capture_hardware_metrics'. You can set them to 'True' when initializing the run and the monitoring will continue until you call run.stop() or the kernel stops. NOTE: To track the source files, pass their paths to the 'source_code' argument. For help, see: https://docs.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/JPL/rna-sequencing/e/RNAS-196\n",
      "Starting Training Loop...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf624b6bd50e4f6cb2b4d2c3deaa8250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "100%|██████████| 4/4 [00:17<00:00,  4.38s/it]\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 D Loss: 0.0021393661732440226 G Loss: 0.14092089402508903 Wasserstein D: -2.142912024384612e-05\n",
      "Epoch 1 D Loss: 0.0012957395373524485 G Loss: 0.09353192739344977 Wasserstein D: -3.511208545911562e-05\n",
      "Epoch 2 D Loss: 0.001123168251731179 G Loss: 0.07730406595693602 Wasserstein D: -0.00013854753199990812\n",
      "Epoch 3 D Loss: 0.0006770808379966896 G Loss: 0.06878841376596398 Wasserstein D: -0.00023808991992390238\n",
      "Epoch 4 D Loss: -0.0002166120843453841 G Loss: 0.06163631457757283 Wasserstein D: -0.000785356113960693\n",
      "Epoch 5 D Loss: -0.0007302447945087939 G Loss: 0.059820024882788425 Wasserstein D: -0.0014415035297820618\n",
      "Epoch 6 D Loss: -0.0011937784981894326 G Loss: 0.11527941250926131 Wasserstein D: -0.00196809368533688\n",
      "Epoch 7 D Loss: -0.002564546528396073 G Loss: 0.2163058628777524 Wasserstein D: -0.003004715992854192\n",
      "Epoch 8 D Loss: -0.003123686030194476 G Loss: 0.36325119737978584 Wasserstein D: -0.0037797239276912663\n",
      "Epoch 9 D Loss: -0.002816650417301205 G Loss: 0.4767960730966154 Wasserstein D: -0.004516207671665645\n",
      "Epoch 10 D Loss: -0.0020783902048231006 G Loss: 0.5379107373577732 Wasserstein D: -0.004533071617980103\n",
      "Epoch 11 D Loss: -0.0037214330859951204 G Loss: 0.4203261474629382 Wasserstein D: -0.004827314323478645\n",
      "Epoch 12 D Loss: -0.0032723262593462752 G Loss: 0.5747382284461201 Wasserstein D: -0.00612588612349717\n",
      "Epoch 13 D Loss: -0.0029023635637510075 G Loss: 0.5629991261692314 Wasserstein D: -0.004745069083633957\n",
      "Epoch 14 D Loss: -0.004425243064240142 G Loss: 0.25903695123595794 Wasserstein D: -0.006210692278988712\n",
      "Epoch 15 D Loss: -0.006570243752086079 G Loss: 0.3372963694008914 Wasserstein D: -0.008147859906816816\n",
      "Epoch 16 D Loss: -0.007989343229707305 G Loss: 0.31089558978597603 Wasserstein D: -0.009422778249620558\n",
      "Epoch 17 D Loss: -0.01265769238238568 G Loss: 0.47065440341309234 Wasserstein D: -0.014235178073803027\n",
      "Epoch 18 D Loss: 0.0004581454750541207 G Loss: -0.33869637283203485 Wasserstein D: -0.002473509395039165\n",
      "Epoch 19 D Loss: -0.002500944412671603 G Loss: -0.4460461770112698 Wasserstein D: -0.0055477210691758806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:17<00:00,  4.36s/it]\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 D Loss: -0.0024069670613829073 G Loss: -0.13730183690525852 Wasserstein D: -0.004706678690610232\n",
      "Epoch 21 D Loss: -0.004447150897312831 G Loss: -0.46317615646582383 Wasserstein D: -0.005851593884554776\n",
      "Epoch 22 D Loss: -0.0069376463656658895 G Loss: -0.32624970126402125 Wasserstein D: -0.007859504306233013\n",
      "Epoch 23 D Loss: -0.002248961191910964 G Loss: -0.05166575771111708 Wasserstein D: -0.005861780860207297\n",
      "Epoch 24 D Loss: 0.005286205065000308 G Loss: 0.030175709693165093 Wasserstein D: -0.007924851837691727\n",
      "Epoch 25 D Loss: -0.0029273404108060824 G Loss: -0.37291872798354475 Wasserstein D: -0.005585281582145424\n",
      "Epoch 26 D Loss: -0.0018617644176616534 G Loss: -0.4676255226343662 Wasserstein D: -0.00619491413756684\n",
      "Epoch 27 D Loss: -0.0048055215315385294 G Loss: -0.4192444775904809 Wasserstein D: -0.006219801786062601\n",
      "Epoch 28 D Loss: -0.0041939245237337126 G Loss: -0.22834747396670022 Wasserstein D: -0.006851506816757309\n",
      "Epoch 29 D Loss: -0.004217569227818843 G Loss: -0.4029840196033458 Wasserstein D: -0.005923939751578377\n",
      "Epoch 30 D Loss: -0.004783709566076318 G Loss: -0.42761690945892067 Wasserstein D: -0.006535852288866376\n",
      "Epoch 31 D Loss: -0.00666216370109078 G Loss: -0.6334804348595493 Wasserstein D: -0.007657501247379329\n",
      "Epoch 32 D Loss: -0.006374018175618632 G Loss: -0.6659182483082885 Wasserstein D: -0.007717521040589659\n",
      "Epoch 33 D Loss: -0.008263883890805545 G Loss: -0.8907203188726118 Wasserstein D: -0.01129874983033934\n",
      "Epoch 34 D Loss: -0.0123637139380395 G Loss: -0.5010575816764699 Wasserstein D: -0.01394718510287625\n",
      "Epoch 35 D Loss: -0.011776497314026305 G Loss: -0.5219964145363628 Wasserstein D: -0.016244151375510475\n",
      "Epoch 36 D Loss: -0.006790225739245648 G Loss: -1.2037207967751509 Wasserstein D: -0.009699020352396932\n",
      "Epoch 37 D Loss: -0.001758924314192125 G Loss: -0.9180144613439386 Wasserstein D: -0.008087518331887839\n",
      "Epoch 38 D Loss: 0.00042816642281058787 G Loss: -1.1783898410263596 Wasserstein D: -0.007187008857727051\n",
      "Epoch 39 D Loss: -0.004369243458434418 G Loss: -1.1194827431565397 Wasserstein D: -0.006071658401222496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:18<00:00,  4.60s/it]\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 D Loss: -0.0011489353813491502 G Loss: -0.8639711249958385 Wasserstein D: -0.007654134210173066\n",
      "Epoch 41 D Loss: -0.00466138374555361 G Loss: -0.9010141808669884 Wasserstein D: -0.007322299730527651\n",
      "Epoch 42 D Loss: -0.009095171948412915 G Loss: -0.8080448249836901 Wasserstein D: -0.010628301780540627\n",
      "Epoch 43 D Loss: -0.0018525111091720475 G Loss: -0.8853536223198151 Wasserstein D: -0.006321156775201117\n",
      "Epoch 44 D Loss: -0.00648728784147676 G Loss: -1.0111792266785682 Wasserstein D: -0.008047766618795329\n",
      "Epoch 45 D Loss: -0.009679600075408296 G Loss: -0.8523195410941864 Wasserstein D: -0.011487485645534276\n",
      "Epoch 46 D Loss: -0.008269731815044697 G Loss: -1.0052254062432509 Wasserstein D: -0.011380919209727041\n",
      "Epoch 47 D Loss: 0.01794618326467234 G Loss: -0.8262450365753441 Wasserstein D: -0.0106625331865324\n",
      "Epoch 48 D Loss: -0.002409216407295707 G Loss: -1.362741699585548 Wasserstein D: -0.008396512144929046\n",
      "Epoch 49 D Loss: 0.003148208964954723 G Loss: -1.0784982897184945 Wasserstein D: -0.007816026260802796\n",
      "Epoch 50 D Loss: -0.006175324633404925 G Loss: -1.0428293468235257 Wasserstein D: -0.007968234849142861\n",
      "Epoch 51 D Loss: -0.003974418123285254 G Loss: -0.9161284353349592 Wasserstein D: -0.010673000262333797\n",
      "Epoch 52 D Loss: 0.012016194683688504 G Loss: -1.0207001892003147 Wasserstein D: -0.00817805903774875\n",
      "Epoch 53 D Loss: -0.007537675070595908 G Loss: -1.0222727718886795 Wasserstein D: -0.010432411740709852\n",
      "Epoch 54 D Loss: -0.007314159319950984 G Loss: -1.098843019742232 Wasserstein D: -0.009211946200657557\n",
      "Epoch 55 D Loss: -0.009227369215104964 G Loss: -0.8609284891948833 Wasserstein D: -0.011597585844826865\n",
      "Epoch 56 D Loss: -0.010806685561066742 G Loss: -0.7905897856592299 Wasserstein D: -0.013085091864312445\n",
      "Epoch 57 D Loss: -0.007865305547114019 G Loss: -0.8986267935145985 Wasserstein D: -0.011191774915148328\n",
      "Epoch 58 D Loss: -0.017775735655031003 G Loss: -0.6059577894377541 Wasserstein D: -0.019289386855972396\n",
      "Epoch 59 D Loss: -0.011647551209776551 G Loss: -1.0092876904702688 Wasserstein D: -0.014094929595093626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:18<00:00,  4.55s/it]\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 D Loss: 0.011882416851870664 G Loss: -0.83280215563474 Wasserstein D: -0.005106472468876339\n",
      "Epoch 61 D Loss: -0.0036728348765339884 G Loss: -1.4803626078825731 Wasserstein D: -0.009049342228816105\n",
      "Epoch 62 D Loss: -0.00876898365420895 G Loss: -1.2279082799291277 Wasserstein D: -0.011014784966315423\n",
      "Epoch 63 D Loss: -0.002843033183704723 G Loss: -1.2725502490163683 Wasserstein D: -0.006492629751458868\n",
      "Epoch 64 D Loss: -0.005322767304373787 G Loss: -1.1756672167277837 Wasserstein D: -0.007425489959183273\n",
      "Epoch 65 D Loss: -0.008197544337986232 G Loss: -1.0866167945461673 Wasserstein D: -0.009541458183235221\n",
      "Epoch 66 D Loss: -0.010137658852797288 G Loss: -0.9785070790277495 Wasserstein D: -0.011605647894052358\n",
      "Epoch 67 D Loss: -0.013534818495903815 G Loss: -0.8618783200537408 Wasserstein D: -0.015553059277834591\n",
      "Epoch 68 D Loss: -0.00458643569812908 G Loss: -1.110693199234409 Wasserstein D: -0.013996117598526961\n",
      "Epoch 69 D Loss: -0.009945407614007697 G Loss: -1.5040984453854862 Wasserstein D: -0.012209734716615477\n",
      "Epoch 70 D Loss: -0.013433114632026299 G Loss: -1.589543654368474 Wasserstein D: -0.015086325732144442\n",
      "Epoch 71 D Loss: -0.002337412400679155 G Loss: -1.5269048330667137 Wasserstein D: -0.01725947940266216\n",
      "Epoch 72 D Loss: -0.013009095525408125 G Loss: -1.6603604138314307 Wasserstein D: -0.016513382638251032\n",
      "Epoch 73 D Loss: -0.0075178037990223275 G Loss: -1.642967146593374 Wasserstein D: -0.011478661657213332\n",
      "Epoch 74 D Loss: -0.006053330181361912 G Loss: -1.8463814308593323 Wasserstein D: -0.011250686812234092\n",
      "Epoch 75 D Loss: 0.025458432577706715 G Loss: -1.9101044389751407 Wasserstein D: -0.007072145288640802\n",
      "Epoch 76 D Loss: -0.0016450244229990285 G Loss: -1.6391307715769414 Wasserstein D: -0.006169519224366942\n",
      "Epoch 77 D Loss: 0.0025493948609678894 G Loss: -1.3580519736229957 Wasserstein D: -0.007566640427062562\n",
      "Epoch 78 D Loss: -0.010135493078431883 G Loss: -0.9123414500609978 Wasserstein D: -0.01229273772739864\n",
      "Epoch 79 D Loss: -0.010569920073022376 G Loss: -0.8026668885787884 Wasserstein D: -0.012748814962960623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:17<00:00,  4.44s/it]\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 D Loss: -0.011367591110976426 G Loss: -0.7415944571261639 Wasserstein D: -0.013654338729965102\n",
      "Epoch 81 D Loss: -0.013016448987947477 G Loss: -0.7778571275147524 Wasserstein D: -0.01531299177583281\n",
      "Epoch 82 D Loss: -0.008050865226692253 G Loss: -1.0084767825119978 Wasserstein D: -0.011545698125879248\n",
      "Epoch 83 D Loss: -0.010587718103315446 G Loss: -1.1447820763487917 Wasserstein D: -0.015302936513940772\n",
      "Epoch 84 D Loss: -0.016574481150487087 G Loss: -0.5909836056765977 Wasserstein D: -0.019539426256726673\n",
      "Epoch 85 D Loss: -0.015369630360103154 G Loss: -0.9328118516848638 Wasserstein D: -0.0178012781209879\n",
      "Epoch 86 D Loss: -0.012723263327058379 G Loss: -0.3871718449192447 Wasserstein D: -0.021015967522467767\n",
      "Epoch 87 D Loss: -0.022891823228422578 G Loss: -0.1325850504991058 Wasserstein D: -0.02508129773440061\n",
      "Epoch 88 D Loss: -0.019992069764570755 G Loss: -0.11803742236347048 Wasserstein D: -0.028849955205317145\n",
      "Epoch 89 D Loss: -0.03017441542832168 G Loss: -0.04854571973006208 Wasserstein D: -0.03357453779740767\n",
      "Epoch 90 D Loss: -0.020705696586128714 G Loss: -0.1698043375451873 Wasserstein D: -0.03477112229887422\n",
      "Epoch 91 D Loss: -0.037418418830924935 G Loss: -0.4702877439819016 Wasserstein D: -0.03973388338422442\n",
      "Epoch 92 D Loss: -0.02987390131383509 G Loss: -0.4639168096172226 Wasserstein D: -0.04608890893576029\n",
      "Epoch 93 D Loss: -0.03165332420722588 G Loss: -0.4787227302998096 Wasserstein D: -0.048824490367115794\n",
      "Epoch 94 D Loss: -0.04312886058033763 G Loss: -0.5631879280497144 Wasserstein D: -0.05078308065454443\n",
      "Epoch 95 D Loss: -0.047993743336284075 G Loss: -0.7664304446507167 Wasserstein D: -0.0510757019469788\n",
      "Epoch 96 D Loss: -0.034269602982314316 G Loss: -0.550793512926235 Wasserstein D: -0.05264033804406653\n",
      "Epoch 97 D Loss: -0.040622674501859225 G Loss: -0.1989647888975752 Wasserstein D: -0.05641383057707673\n",
      "Epoch 98 D Loss: 0.006687014252989443 G Loss: 0.11027627394020141 Wasserstein D: -0.05305453947374037\n",
      "Epoch 99 D Loss: -0.038070795419332865 G Loss: 0.06608200801367109 Wasserstein D: -0.05361313253015905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:18<00:00,  4.57s/it]\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 D Loss: -0.031253851377047025 G Loss: 0.5249403069590355 Wasserstein D: -0.05870729726511282\n",
      "Epoch 101 D Loss: -0.0259581012325687 G Loss: 1.2214605799921743 Wasserstein D: -0.05805141609031837\n",
      "Epoch 102 D Loss: -0.06325993838010134 G Loss: 1.7582153682108526 Wasserstein D: -0.067701393074089\n",
      "Epoch 103 D Loss: -0.059483361410927936 G Loss: 2.388074234649018 Wasserstein D: -0.07178728063623388\n",
      "Epoch 104 D Loss: -0.06320743960934086 G Loss: 2.571054466954478 Wasserstein D: -0.07755552972113336\n",
      "Epoch 105 D Loss: -0.07529606185592971 G Loss: 2.6154433380473745 Wasserstein D: -0.07939392036491341\n",
      "Epoch 106 D Loss: -0.05349902839927407 G Loss: 2.68593915609213 Wasserstein D: -0.082024567610734\n",
      "Epoch 107 D Loss: -0.051957847355129004 G Loss: 2.7004801660150917 Wasserstein D: -0.0840218150532329\n",
      "Epoch 108 D Loss: -0.06772270069255695 G Loss: 3.284835475308078 Wasserstein D: -0.09030987666203426\n",
      "Epoch 109 D Loss: -0.08022026248745151 G Loss: 2.688414928796408 Wasserstein D: -0.0846299991741047\n",
      "Epoch 110 D Loss: -0.0765901045365767 G Loss: 3.302130365705157 Wasserstein D: -0.08893247751089242\n",
      "Epoch 111 D Loss: -0.07081085151725716 G Loss: 3.389462612725638 Wasserstein D: -0.09112662868899898\n",
      "Epoch 112 D Loss: -0.08829198850618376 G Loss: 3.83659687909213 Wasserstein D: -0.09863618703988883\n",
      "Epoch 113 D Loss: -0.0841583905520139 G Loss: 4.35199249374283 Wasserstein D: -0.10260217840021307\n",
      "Epoch 114 D Loss: -0.08942798801235385 G Loss: 5.026067366966834 Wasserstein D: -0.09524170001903615\n",
      "Epoch 115 D Loss: -0.03834849804431408 G Loss: 4.864305316151439 Wasserstein D: -0.08486014146071214\n",
      "Epoch 116 D Loss: -0.052127838134765625 G Loss: 3.4455529776486484 Wasserstein D: -0.09358872900475988\n",
      "Epoch 117 D Loss: -0.0746670569573249 G Loss: 3.9065268323138045 Wasserstein D: -0.0956201486654215\n",
      "Epoch 118 D Loss: -0.09485178727370042 G Loss: 4.3530120149359 Wasserstein D: -0.09991117624136117\n",
      "Epoch 119 D Loss: -0.06279085065935042 G Loss: 4.561595761692607 Wasserstein D: -0.10633996816781852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:18<00:00,  4.54s/it]\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120 D Loss: -0.10570460766345471 G Loss: 5.0592813225059245 Wasserstein D: -0.11116085852776374\n",
      "Epoch 121 D Loss: -0.10218951085230688 G Loss: 5.282648823478005 Wasserstein D: -0.11312615454613746\n",
      "Epoch 122 D Loss: 0.06327881846394572 G Loss: 5.493449171106298 Wasserstein D: -0.08696620781104882\n",
      "Epoch 123 D Loss: -0.0737147698035607 G Loss: 3.5562130771316847 Wasserstein D: -0.10241836601203971\n",
      "Epoch 124 D Loss: -0.08047657413082523 G Loss: 4.357551533025461 Wasserstein D: -0.09951312058455461\n",
      "Epoch 125 D Loss: -0.10361164599865466 G Loss: 4.7545375423831535 Wasserstein D: -0.10753470867663831\n",
      "Epoch 126 D Loss: -0.04957977374950489 G Loss: 5.0587183678900445 Wasserstein D: -0.11238066133085664\n",
      "Epoch 127 D Loss: -0.10067366219900704 G Loss: 5.291641098636013 Wasserstein D: -0.11068631552316092\n",
      "Epoch 128 D Loss: -0.10288758711381392 G Loss: 5.522519535118049 Wasserstein D: -0.11703249791285375\n",
      "Epoch 129 D Loss: -0.10315377562196104 G Loss: 5.59634301712463 Wasserstein D: -0.12044695874194165\n",
      "Epoch 130 D Loss: -0.06523122987547121 G Loss: 5.7268717305643575 Wasserstein D: -0.11708391629732572\n",
      "Epoch 131 D Loss: -0.04225499479920714 G Loss: 5.922678327226972 Wasserstein D: -0.12190820120431327\n",
      "Epoch 132 D Loss: -0.11035981878534064 G Loss: 5.662814607153406 Wasserstein D: -0.11534486783967986\n",
      "Epoch 133 D Loss: -0.10648321938681436 G Loss: 5.410745815797285 Wasserstein D: -0.11824737228713669\n",
      "Epoch 134 D Loss: -0.10974127762801164 G Loss: 5.323539660527156 Wasserstein D: -0.12126055630770596\n",
      "Epoch 135 D Loss: -0.10568722811612216 G Loss: 5.31048913768955 Wasserstein D: -0.1119078122652494\n",
      "Epoch 136 D Loss: -0.08457760043911167 G Loss: 5.587300890809172 Wasserstein D: -0.11957043200939685\n",
      "Epoch 137 D Loss: -0.09907095582335146 G Loss: 5.140782224548446 Wasserstein D: -0.11490551074901661\n",
      "Epoch 138 D Loss: -0.08011763245909365 G Loss: 5.647933693198891 Wasserstein D: -0.1194226458356097\n",
      "Epoch 139 D Loss: -0.10466461581783695 G Loss: 5.354311756320767 Wasserstein D: -0.1243938366016308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:18<00:00,  4.52s/it]\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140 D Loss: -0.08347616662512293 G Loss: 5.85018164294583 Wasserstein D: -0.12188758049811517\n",
      "Epoch 141 D Loss: -0.06574420662193031 G Loss: 6.118104361153983 Wasserstein D: -0.1311111716957359\n",
      "Epoch 142 D Loss: -0.08604805286114033 G Loss: 6.269702231133734 Wasserstein D: -0.12780765720180698\n",
      "Epoch 143 D Loss: -0.07694184363305152 G Loss: 6.506160049171714 Wasserstein D: -0.1166980449969952\n",
      "Epoch 144 D Loss: -0.08029134790380518 G Loss: 6.259009798089941 Wasserstein D: -0.12351192127574574\n",
      "Epoch 145 D Loss: -0.010916990000051219 G Loss: 6.208969629727877 Wasserstein D: -0.10572149370100115\n",
      "Epoch 146 D Loss: -0.056302257351108366 G Loss: 6.109132269879321 Wasserstein D: -0.1256523265705242\n",
      "Epoch 147 D Loss: -0.1127739086017742 G Loss: 6.177499257601225 Wasserstein D: -0.12084012598424525\n",
      "Epoch 148 D Loss: 0.253821499697812 G Loss: 6.3887050702021675 Wasserstein D: -0.12963226958588286\n",
      "Epoch 149 D Loss: -0.002963579618013822 G Loss: 6.561809249691196 Wasserstein D: -0.12476511601801518\n",
      "Epoch 150 D Loss: 0.07373538384070763 G Loss: 7.066954035859008 Wasserstein D: -0.10428021170876244\n",
      "Epoch 151 D Loss: 0.3908652192229158 G Loss: 7.49093805659901 Wasserstein D: -0.12582330770425862\n",
      "Epoch 152 D Loss: -0.08661532902217411 G Loss: 3.8619408257357724 Wasserstein D: -0.10683544532402411\n",
      "Epoch 153 D Loss: -0.0996250005868765 G Loss: 4.365079424598 Wasserstein D: -0.11063156261310711\n",
      "Epoch 154 D Loss: -0.09849156866540441 G Loss: 4.91352919932012 Wasserstein D: -0.11799072052215363\n",
      "Epoch 155 D Loss: -0.09101592910873306 G Loss: 5.465048633255325 Wasserstein D: -0.09893532732983569\n",
      "Epoch 156 D Loss: 2.3039969064138988 G Loss: 5.025345919849156 Wasserstein D: -0.04515255747975169\n",
      "Epoch 157 D Loss: 3.272207833670236 G Loss: 1.7323038580117527 Wasserstein D: -0.019236883083423535\n",
      "Epoch 158 D Loss: 1.6615879085514096 G Loss: 0.9008720672422356 Wasserstein D: -0.032926296020721224\n",
      "Epoch 159 D Loss: 2.601342261254371 G Loss: 0.8083460991094996 Wasserstein D: -0.02922778029541869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:18<00:00,  4.58s/it]\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160 D Loss: 4.1560442731097025 G Loss: -0.9778041423695696 Wasserstein D: -0.024244336815147132\n",
      "Epoch 161 D Loss: 0.49252959564849214 G Loss: -1.0415555110552928 Wasserstein D: -0.022978615927529502\n",
      "Epoch 162 D Loss: 0.603136662836675 G Loss: -1.6613944122841309 Wasserstein D: -0.007851321380455177\n",
      "Epoch 163 D Loss: 0.6325238634656359 G Loss: -2.1045269174175663 Wasserstein D: -0.008278695019808683\n",
      "Epoch 164 D Loss: 0.6392990859238418 G Loss: -2.398957667650876 Wasserstein D: -0.01091045266264802\n",
      "Epoch 165 D Loss: 0.08712614499605618 G Loss: -2.9187320845944065 Wasserstein D: -0.007535861088679387\n",
      "Epoch 166 D Loss: 0.13020125969306573 G Loss: -2.8316095575586067 Wasserstein D: -0.003268115170352109\n",
      "Epoch 167 D Loss: 0.1804401557762306 G Loss: -2.795582594571414 Wasserstein D: -0.004019715569236062\n",
      "Epoch 168 D Loss: 0.21230233465874945 G Loss: -2.759952558504118 Wasserstein D: -0.0038467553945688102\n",
      "Epoch 169 D Loss: 0.06908149986000328 G Loss: -1.878341292167877 Wasserstein D: -0.003651959079128879\n",
      "Epoch 170 D Loss: 0.03865217995810342 G Loss: -0.9963958292574315 Wasserstein D: -0.005444711738533074\n",
      "Epoch 171 D Loss: 0.0131167748591283 G Loss: -0.6642659168977004 Wasserstein D: -0.004163882532319823\n",
      "Epoch 172 D Loss: -0.003674482548987115 G Loss: -0.48282416091932284 Wasserstein D: -0.008231834931807085\n",
      "Epoch 173 D Loss: -0.005455535191756029 G Loss: -0.09894174760715528 Wasserstein D: -0.008816095498891978\n",
      "Epoch 174 D Loss: -0.0016425001871335756 G Loss: -0.016851617511804706 Wasserstein D: -0.008203298061877697\n",
      "Epoch 175 D Loss: -0.01087245824453714 G Loss: 0.15412062971194931 Wasserstein D: -0.014108844570346646\n",
      "Epoch 176 D Loss: -0.011989450121259355 G Loss: 0.7260074413322902 Wasserstein D: -0.017333012360792894\n",
      "Epoch 177 D Loss: -0.016907306817861702 G Loss: 0.7081633178921013 Wasserstein D: -0.020393308226045195\n",
      "Epoch 178 D Loss: -0.016553623692972676 G Loss: 0.6103059947490692 Wasserstein D: -0.020377607612343102\n",
      "Epoch 179 D Loss: -0.017400491487729798 G Loss: 0.23425331156883206 Wasserstein D: -0.021252128627750423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:18<00:00,  4.51s/it]\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180 D Loss: -0.027338134658920182 G Loss: 0.19177450558782874 Wasserstein D: -0.029521998825606768\n",
      "Epoch 181 D Loss: -0.0299168299961757 G Loss: 0.06630244502680493 Wasserstein D: -0.03276510505409507\n",
      "Epoch 182 D Loss: -0.03510809945059823 G Loss: -0.3590518906973042 Wasserstein D: -0.03866434097290039\n",
      "Epoch 183 D Loss: -0.0388967474023779 G Loss: -0.7606660173489497 Wasserstein D: -0.044717855386800696\n",
      "Epoch 184 D Loss: -0.03966474533081055 G Loss: -1.2344332520778363 Wasserstein D: -0.04766378202638426\n",
      "Epoch 185 D Loss: -0.036589249030693424 G Loss: -1.5114675418480292 Wasserstein D: -0.049871734805874056\n",
      "Epoch 186 D Loss: -0.03352658398501523 G Loss: -2.4065933169184865 Wasserstein D: -0.05600050279310533\n",
      "Epoch 187 D Loss: -0.05151104760336709 G Loss: -3.2116882084132907 Wasserstein D: -0.06262241710316051\n",
      "Epoch 188 D Loss: -0.0498475995097127 G Loss: -3.2183462806514926 Wasserstein D: -0.06100302476149339\n",
      "Epoch 189 D Loss: -0.0700100785368806 G Loss: -3.605651416978636 Wasserstein D: -0.07872793557760599\n",
      "Epoch 190 D Loss: -0.06936214687107327 G Loss: -3.9054703479046586 Wasserstein D: -0.0820719578883031\n",
      "Epoch 191 D Loss: 0.22604183383754917 G Loss: -6.050883116422 Wasserstein D: -0.024099089882590553\n",
      "Epoch 192 D Loss: -0.003975814872688347 G Loss: -8.340296118409483 Wasserstein D: -0.02871443008209442\n",
      "Epoch 193 D Loss: -0.02672350490009868 G Loss: -7.485209188261233 Wasserstein D: -0.03803369215318373\n",
      "Epoch 194 D Loss: 0.04192412316382348 G Loss: -7.322190341415939 Wasserstein D: -0.03645736187488049\n",
      "Epoch 195 D Loss: -0.014891632786997549 G Loss: -8.322534734552557 Wasserstein D: -0.03314287679178731\n",
      "Epoch 196 D Loss: -0.03583467923677885 G Loss: -8.40430854250501 Wasserstein D: -0.043438834743899896\n",
      "Epoch 197 D Loss: -0.04083631088683655 G Loss: -7.707581099930343 Wasserstein D: -0.046762993285705996\n",
      "Epoch 198 D Loss: -0.04533617979996688 G Loss: -7.417711858149175 Wasserstein D: -0.05192151436438927\n",
      "Epoch 199 D Loss: -0.03849758134855257 G Loss: -7.471768159132737 Wasserstein D: -0.046697943360655456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:18<00:00,  4.54s/it]\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200 D Loss: -0.050758425172392305 G Loss: -7.8745068436736 Wasserstein D: -0.05463692191597465\n",
      "Epoch 201 D Loss: -0.0575090354972786 G Loss: -8.362173540608866 Wasserstein D: -0.06191073597727956\n",
      "Epoch 202 D Loss: -0.06268760707828548 G Loss: -8.066839978411481 Wasserstein D: -0.06752548351154461\n",
      "Epoch 203 D Loss: -0.06722394236317882 G Loss: -8.155716055756683 Wasserstein D: -0.07878446912432051\n",
      "Epoch 204 D Loss: 0.003156715339713997 G Loss: -10.023569533874939 Wasserstein D: -0.04661848161604021\n",
      "Epoch 205 D Loss: -0.026230935450200433 G Loss: -12.114214203574441 Wasserstein D: -0.049990140474759616\n",
      "Epoch 206 D Loss: -0.05136468860652897 G Loss: -12.47672640527045 Wasserstein D: -0.058535282428448014\n",
      "Epoch 207 D Loss: -0.06598308536556217 G Loss: -10.522477803530393 Wasserstein D: -0.07436250806688428\n",
      "Epoch 208 D Loss: -0.02833141980471311 G Loss: -11.53078728122311 Wasserstein D: -0.06014251708984375\n",
      "Epoch 209 D Loss: -0.054941704223205996 G Loss: -12.350418604337252 Wasserstein D: -0.06233786869715977\n",
      "Epoch 210 D Loss: -0.06437026870834243 G Loss: -11.873479929837314 Wasserstein D: -0.07503604888916016\n",
      "Epoch 211 D Loss: -0.06611712662490098 G Loss: -13.187285343250194 Wasserstein D: -0.07164747064763849\n",
      "Epoch 212 D Loss: -0.028169438555524066 G Loss: -15.24612607489099 Wasserstein D: -0.04624360091202743\n",
      "Epoch 213 D Loss: -0.06345688879906715 G Loss: -14.42328723827442 Wasserstein D: -0.07193904156451458\n",
      "Epoch 214 D Loss: -0.022979751333490117 G Loss: -13.422974753213095 Wasserstein D: -0.04198177044208233\n",
      "Epoch 215 D Loss: 0.0034503103136182666 G Loss: -16.320467208648896 Wasserstein D: -0.04244895081420045\n",
      "Epoch 216 D Loss: -0.051209579814564095 G Loss: -15.124239694822085 Wasserstein D: -0.05735526718459763\n",
      "Epoch 217 D Loss: -0.05708691790387347 G Loss: -14.763813745725406 Wasserstein D: -0.0658977148416159\n",
      "Epoch 218 D Loss: -0.05131169632598237 G Loss: -14.162416744899083 Wasserstein D: -0.069583052521819\n",
      "Epoch 219 D Loss: -0.051948390640578906 G Loss: -14.086309653062086 Wasserstein D: -0.0770886494563176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:18<00:00,  4.51s/it]\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220 D Loss: -0.0642902000800713 G Loss: -14.002759259897513 Wasserstein D: -0.08319441922061094\n",
      "Epoch 221 D Loss: 0.265903446224186 G Loss: -15.283032904138098 Wasserstein D: -0.032992369645125384\n",
      "Epoch 222 D Loss: -0.008178020690704559 G Loss: -17.877082331197244 Wasserstein D: -0.049953193931312825\n",
      "Epoch 223 D Loss: -0.048621871254660866 G Loss: -17.993089222407843 Wasserstein D: -0.05473791802679742\n",
      "Epoch 224 D Loss: -0.061313735855209245 G Loss: -17.654340170480154 Wasserstein D: -0.06573689067280376\n",
      "Epoch 225 D Loss: -0.07610010934042764 G Loss: -15.46330221883067 Wasserstein D: -0.0801100030645624\n",
      "Epoch 226 D Loss: -0.08558894204093026 G Loss: -14.865056851527074 Wasserstein D: -0.08940793417550467\n",
      "Epoch 227 D Loss: -0.07388525742750901 G Loss: -14.95514271476052 Wasserstein D: -0.08939262203403286\n",
      "Epoch 228 D Loss: 0.11960659160480633 G Loss: -15.001110877190436 Wasserstein D: -0.08139202144596126\n",
      "Epoch 229 D Loss: -0.06609381162203275 G Loss: -18.91507150076486 Wasserstein D: -0.07500591144695148\n",
      "Epoch 230 D Loss: 0.003077913831163953 G Loss: -18.132780501892515 Wasserstein D: -0.05488781162075229\n",
      "Epoch 231 D Loss: 0.022469293821108093 G Loss: -19.743997387119105 Wasserstein D: -0.041760744748415644\n",
      "Epoch 232 D Loss: -0.03879347047605715 G Loss: -19.427246293821536 Wasserstein D: -0.05495030063015598\n",
      "Epoch 233 D Loss: -0.059323337528255436 G Loss: -19.04631290569172 Wasserstein D: -0.0667406762396539\n",
      "Epoch 234 D Loss: -0.06481932926844884 G Loss: -18.095672273969317 Wasserstein D: -0.07259690344750465\n",
      "Epoch 235 D Loss: -0.07800885180493335 G Loss: -17.67370570789684 Wasserstein D: -0.08320808410644531\n",
      "Epoch 236 D Loss: -0.07867390959412901 G Loss: -17.211537754619037 Wasserstein D: -0.08815094474312309\n",
      "Epoch 237 D Loss: -0.07861877654815887 G Loss: -17.298738159499802 Wasserstein D: -0.09324137814395078\n",
      "Epoch 238 D Loss: -0.08572554421591592 G Loss: -17.540735444822513 Wasserstein D: -0.09371166629391117\n",
      "Epoch 239 D Loss: -0.0966820016607538 G Loss: -18.014431333208417 Wasserstein D: -0.10124554667439493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:18<00:00,  4.57s/it]\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240 D Loss: -0.0983684379737694 G Loss: -17.784092429634576 Wasserstein D: -0.10507539602426383\n",
      "Epoch 241 D Loss: 0.011095793930800645 G Loss: -21.594510938737777 Wasserstein D: -0.07102452791654147\n",
      "Epoch 242 D Loss: -0.02033135607526019 G Loss: -21.938126050508938 Wasserstein D: -0.07783980469603638\n",
      "Epoch 243 D Loss: -0.062492163864882676 G Loss: -22.269845402324115 Wasserstein D: -0.07701719057309878\n",
      "Epoch 244 D Loss: -0.049916100668740436 G Loss: -22.408604028341653 Wasserstein D: -0.08558617438469733\n",
      "Epoch 245 D Loss: -0.06856486180445531 G Loss: -22.288875979977053 Wasserstein D: -0.09148860477901005\n",
      "Epoch 246 D Loss: -0.08835003592751244 G Loss: -22.075004257522263 Wasserstein D: -0.0990977920852341\n",
      "Epoch 247 D Loss: -0.04236506081961252 G Loss: -21.98283352885213 Wasserstein D: -0.1042775374192458\n",
      "Epoch 248 D Loss: -0.11328970635687555 G Loss: -21.425038597800516 Wasserstein D: -0.1181138978971468\n",
      "Epoch 249 D Loss: -0.10864261147025582 G Loss: -21.038857973538914 Wasserstein D: -0.11737607409070422\n",
      "Epoch 250 D Loss: 0.06882224716506638 G Loss: -21.470165692842922 Wasserstein D: -0.12921119903351044\n",
      "Epoch 251 D Loss: -0.12654035074727518 G Loss: -21.66488340684584 Wasserstein D: -0.13646677990893383\n",
      "Epoch 252 D Loss: -0.04293136996822757 G Loss: -27.41550985749785 Wasserstein D: -0.0887521863817335\n",
      "Epoch 253 D Loss: -0.08335085515375737 G Loss: -27.380467941711 Wasserstein D: -0.09073301462026742\n",
      "Epoch 254 D Loss: 0.04119428888067499 G Loss: -23.239655781459142 Wasserstein D: -0.12922325667801438\n",
      "Epoch 255 D Loss: -0.09585127797160116 G Loss: -23.693876506565335 Wasserstein D: -0.11353710147884342\n",
      "Epoch 256 D Loss: -0.1103770115992406 G Loss: -24.83952023432805 Wasserstein D: -0.11603827576537232\n",
      "Epoch 257 D Loss: -0.10866828064818482 G Loss: -24.71479321193028 Wasserstein D: -0.12300452652511064\n",
      "Epoch 258 D Loss: -0.12306472138091401 G Loss: -25.14514606982678 Wasserstein D: -0.13222533005934495\n",
      "Epoch 259 D Loss: -0.14332627249764396 G Loss: -25.554696116414103 Wasserstein D: -0.15046793264108937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:18<00:00,  4.54s/it]\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 260 D Loss: 0.038978409933877155 G Loss: -31.54672187858528 Wasserstein D: -0.06341419353351727\n",
      "Epoch 261 D Loss: -0.05813131132325926 G Loss: -34.474427443284256 Wasserstein D: -0.08935194749098557\n",
      "Epoch 262 D Loss: 0.6361581228829764 G Loss: -31.293971308461437 Wasserstein D: -0.07433220389839652\n",
      "Epoch 263 D Loss: -0.11117238931722574 G Loss: -28.82467471302806 Wasserstein D: -0.1352543464073768\n",
      "Epoch 264 D Loss: -0.1014718275803786 G Loss: -28.92845205827193 Wasserstein D: -0.11711480734231589\n",
      "Epoch 265 D Loss: -0.08822866586538461 G Loss: -28.643989936455146 Wasserstein D: -0.1245706064717753\n",
      "Epoch 266 D Loss: -0.11886247221406523 G Loss: -28.898797335324588 Wasserstein D: -0.13697574855564357\n",
      "Epoch 267 D Loss: -0.14754570114029036 G Loss: -28.299487280678918 Wasserstein D: -0.15430485118519177\n",
      "Epoch 268 D Loss: -0.1362174107478215 G Loss: -28.526448603276606 Wasserstein D: -0.14824739202752812\n",
      "Epoch 269 D Loss: -0.14367566408810917 G Loss: -29.759570448548644 Wasserstein D: -0.15693057667125354\n",
      "Epoch 270 D Loss: -0.10278286300339065 G Loss: -30.036247786942063 Wasserstein D: -0.1317655389959162\n",
      "Epoch 271 D Loss: -0.16845543067772073 G Loss: -31.01093004133318 Wasserstein D: -0.17837384363988062\n",
      "Epoch 272 D Loss: -0.1830914770806586 G Loss: -30.769669406063908 Wasserstein D: -0.19078830905727573\n",
      "Epoch 273 D Loss: -0.10233649007090322 G Loss: -36.97799157095956 Wasserstein D: -0.12638145393424935\n",
      "Epoch 274 D Loss: -0.10523744062943892 G Loss: -38.9726190366945 Wasserstein D: -0.1168531671270624\n",
      "Epoch 275 D Loss: -0.14902773770419034 G Loss: -36.48266121390816 Wasserstein D: -0.16836953329873253\n",
      "Epoch 276 D Loss: 0.8512707690258959 G Loss: -34.45909700193605 Wasserstein D: -0.19268177272556544\n",
      "Epoch 277 D Loss: -0.1795521182613773 G Loss: -33.1708554888105 Wasserstein D: -0.19788883449314357\n",
      "Epoch 278 D Loss: -0.11063056892448372 G Loss: -32.96281718540858 Wasserstein D: -0.18236965899700885\n",
      "Epoch 279 D Loss: -0.1954854151585719 G Loss: -33.540819074724105 Wasserstein D: -0.2035548336856015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:18<00:00,  4.53s/it]\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 280 D Loss: -0.04953074955440068 G Loss: -35.60151154844911 Wasserstein D: -0.1504589160839161\n",
      "Epoch 281 D Loss: -0.13516352726862982 G Loss: -37.52738859270003 Wasserstein D: -0.18539874203555234\n",
      "Epoch 282 D Loss: -0.18283118401374016 G Loss: -37.904958258142 Wasserstein D: -0.19306593674879807\n",
      "Epoch 283 D Loss: -0.20026665800934904 G Loss: -37.796471575757 Wasserstein D: -0.21507129802570477\n",
      "Epoch 284 D Loss: -0.20646467408933838 G Loss: -38.808933791580735 Wasserstein D: -0.21735091309447388\n",
      "Epoch 285 D Loss: -0.19782667893629807 G Loss: -39.671666098641346 Wasserstein D: -0.23318091972724542\n",
      "Epoch 286 D Loss: -0.17724134538557146 G Loss: -41.92337700370308 Wasserstein D: -0.1922527393261036\n",
      "Epoch 287 D Loss: -0.21667372430121148 G Loss: -42.66057522647031 Wasserstein D: -0.23629197874269287\n",
      "Epoch 288 D Loss: -0.24937446967705146 G Loss: -41.735956098649886 Wasserstein D: -0.2543067932128906\n",
      "Epoch 289 D Loss: 0.02413683004312582 G Loss: -42.984232549067144 Wasserstein D: -0.16979772394353693\n",
      "Epoch 290 D Loss: -0.23588639372712247 G Loss: -43.554277113267595 Wasserstein D: -0.25278203470723615\n",
      "Epoch 291 D Loss: -0.13110194172892536 G Loss: -57.521791471467985 Wasserstein D: -0.15754280890618172\n",
      "Epoch 292 D Loss: -0.16572294702063073 G Loss: -54.857136572991216 Wasserstein D: -0.18912887573242188\n",
      "Epoch 293 D Loss: -0.19204362455781523 G Loss: -44.359772742211405 Wasserstein D: -0.2634395786098667\n",
      "Epoch 294 D Loss: -0.17254918772023875 G Loss: -46.26864226547988 Wasserstein D: -0.2232567046905731\n",
      "Epoch 295 D Loss: -0.24830099252554086 G Loss: -45.5537877382932 Wasserstein D: -0.2596292295656004\n",
      "Epoch 296 D Loss: -0.1572382733538434 G Loss: -52.45275772201431 Wasserstein D: -0.22623891763753826\n",
      "Epoch 297 D Loss: -0.24684220427399747 G Loss: -50.038839433576676 Wasserstein D: -0.2561278443236451\n",
      "Epoch 298 D Loss: -0.2173480053881665 G Loss: -47.733766329038396 Wasserstein D: -0.2690470768855168\n",
      "Epoch 299 D Loss: -0.20940107065480906 G Loss: -48.12197465163011 Wasserstein D: -0.21738007018616148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:18<00:00,  4.54s/it]\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300 D Loss: -0.16938456288584464 G Loss: -49.97828484915353 Wasserstein D: -0.25332454868129917\n",
      "Epoch 301 D Loss: -0.19488596082567336 G Loss: -59.57637928249119 Wasserstein D: -0.20577512087521854\n",
      "Epoch 302 D Loss: -0.1801435163804701 G Loss: -58.0683725263689 Wasserstein D: -0.19072624686714654\n",
      "Epoch 303 D Loss: -0.21899488755873034 G Loss: -55.37065004468798 Wasserstein D: -0.2720547922841319\n",
      "Epoch 304 D Loss: -0.26251383427973396 G Loss: -51.154060763912604 Wasserstein D: -0.29181948575106537\n",
      "Epoch 305 D Loss: -0.0944535582215636 G Loss: -50.78047281545359 Wasserstein D: -0.2708588180008468\n",
      "Epoch 306 D Loss: -0.21638354054697745 G Loss: -51.832433020318305 Wasserstein D: -0.2670052001526306\n",
      "Epoch 307 D Loss: -0.17858826697289526 G Loss: -60.34751243858071 Wasserstein D: -0.2014572036849869\n",
      "Epoch 308 D Loss: -0.25230839869359156 G Loss: -56.39716739254398 Wasserstein D: -0.29047740589488635\n",
      "Epoch 309 D Loss: 0.005696863561243444 G Loss: -51.435588463203054 Wasserstein D: -0.16994657716551026\n",
      "Epoch 310 D Loss: -0.10751229399567717 G Loss: -63.85942187009158 Wasserstein D: -0.19869056114783654\n",
      "Epoch 311 D Loss: -0.23890638017987872 G Loss: -59.51298109468046 Wasserstein D: -0.2753589870212795\n",
      "Epoch 312 D Loss: -0.17975651134144177 G Loss: -59.9136670519422 Wasserstein D: -0.24244255119270378\n",
      "Epoch 313 D Loss: -0.26628595632273 G Loss: -55.388734270642686 Wasserstein D: -0.3035831051272946\n",
      "Epoch 314 D Loss: -0.20006282512958234 G Loss: -57.78539081386753 Wasserstein D: -0.24643507203855713\n",
      "Epoch 315 D Loss: -0.25238434584824354 G Loss: -60.61035969874242 Wasserstein D: -0.2714836547424743\n",
      "Epoch 316 D Loss: -0.26590227247118114 G Loss: -59.15846433839598 Wasserstein D: -0.27917803250826323\n",
      "Epoch 317 D Loss: 3.2455062332686846 G Loss: -58.252937103484896 Wasserstein D: -0.12686536028668596\n",
      "Epoch 318 D Loss: 0.10235246244844023 G Loss: -56.09058617545175 Wasserstein D: -0.3224996553434359\n",
      "Epoch 319 D Loss: -0.31280114767434714 G Loss: -55.542893016254986 Wasserstein D: -0.3330659933023519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:18<00:00,  4.53s/it]\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 320 D Loss: 49.85796820367133 G Loss: -56.146821828988884 Wasserstein D: -0.18061124361478365\n",
      "Epoch 321 D Loss: -0.2546737777603256 G Loss: -71.82153853836593 Wasserstein D: -0.2700476479697061\n",
      "Epoch 322 D Loss: -0.32524554379336484 G Loss: -58.41647328196706 Wasserstein D: -0.3371375023901879\n",
      "Epoch 323 D Loss: -0.28489887797749125 G Loss: -56.368867674073975 Wasserstein D: -0.3429134875744373\n",
      "Epoch 324 D Loss: -0.06430313803932884 G Loss: -73.03310076840275 Wasserstein D: -0.23191251954832276\n",
      "Epoch 325 D Loss: -0.23160334233637456 G Loss: -69.95351463264518 Wasserstein D: -0.2469093482811134\n",
      "Epoch 326 D Loss: -0.24243580211292615 G Loss: -60.673370361328125 Wasserstein D: -0.30460085568728146\n",
      "Epoch 327 D Loss: -0.16418219613028573 G Loss: -60.827424536218174 Wasserstein D: -0.2914694939459954\n",
      "Epoch 328 D Loss: -0.232536529327606 G Loss: -59.4264397254357 Wasserstein D: -0.3572307666698536\n",
      "Epoch 329 D Loss: -0.31811093950605057 G Loss: -59.15098281006713 Wasserstein D: -0.3577503124317089\n",
      "Epoch 330 D Loss: -0.2062440352006392 G Loss: -66.1942952562879 Wasserstein D: -0.22596361920550154\n",
      "Epoch 331 D Loss: -0.1249175171752076 G Loss: -73.5849643520542 Wasserstein D: -0.26277320701759177\n",
      "Epoch 332 D Loss: -0.23555512861772018 G Loss: -74.62430305747719 Wasserstein D: -0.2817148595423132\n",
      "Epoch 333 D Loss: -0.2909806791719023 G Loss: -63.98090973433915 Wasserstein D: -0.3545627193851071\n",
      "Epoch 334 D Loss: -0.3121998393452251 G Loss: -62.43443495743758 Wasserstein D: -0.36124054702011854\n",
      "Epoch 335 D Loss: -0.18299362876198508 G Loss: -76.76081119884144 Wasserstein D: -0.2288094900704764\n",
      "Epoch 336 D Loss: -0.2899845630138904 G Loss: -65.95857206758086 Wasserstein D: -0.34659888527610083\n",
      "Epoch 337 D Loss: -0.31768742808095224 G Loss: -62.652604723310134 Wasserstein D: -0.37914460188858995\n",
      "Epoch 338 D Loss: -0.295699646422913 G Loss: -62.92909192705488 Wasserstein D: -0.38155364990234375\n",
      "Epoch 339 D Loss: -0.287359464418638 G Loss: -61.80855149489183 Wasserstein D: -0.3632998166384397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:18<00:00,  4.57s/it]\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 340 D Loss: 1.0638393588833042 G Loss: -68.76079746059604 Wasserstein D: -0.188448285723066\n",
      "Epoch 341 D Loss: -0.14059972429608966 G Loss: -84.52130164299811 Wasserstein D: -0.2543792191085282\n",
      "Epoch 342 D Loss: -0.14804534645347328 G Loss: -74.8486448701445 Wasserstein D: -0.3092968814022891\n",
      "Epoch 343 D Loss: -0.24546387145569273 G Loss: -69.61268775779884 Wasserstein D: -0.36222866031673406\n",
      "Epoch 344 D Loss: -0.24909775740616805 G Loss: -67.9438304767742 Wasserstein D: -0.3489769355400459\n",
      "Epoch 345 D Loss: 0.6085742336886746 G Loss: -70.22031365241205 Wasserstein D: -0.26056217647098995\n",
      "Epoch 346 D Loss: -0.30807743205890786 G Loss: -74.74856241933115 Wasserstein D: -0.3321420096017264\n",
      "Epoch 347 D Loss: -0.30515438693386693 G Loss: -75.20105972823563 Wasserstein D: -0.3454304408360194\n",
      "Epoch 348 D Loss: -0.34842756578138656 G Loss: -71.82839987161276 Wasserstein D: -0.38366299075680177\n",
      "Epoch 349 D Loss: -0.34838888528463724 G Loss: -69.6537387020938 Wasserstein D: -0.3964583923766663\n",
      "Epoch 350 D Loss: -0.1066475314693851 G Loss: -86.3167839317055 Wasserstein D: -0.2693843708171711\n",
      "Epoch 351 D Loss: -0.06012226985051082 G Loss: -88.91661946756857 Wasserstein D: -0.24316726364455857\n",
      "Epoch 352 D Loss: -0.26670226517257156 G Loss: -89.16306150209654 Wasserstein D: -0.32050894357107734\n",
      "Epoch 353 D Loss: -0.3449762517755682 G Loss: -76.86017069783244 Wasserstein D: -0.3736953201827469\n",
      "Epoch 354 D Loss: -0.3793293879582332 G Loss: -75.82512008560288 Wasserstein D: -0.3912115030355387\n",
      "Epoch 355 D Loss: -0.34990681468190016 G Loss: -75.46272309843476 Wasserstein D: -0.36976527500819495\n",
      "Epoch 356 D Loss: -0.3315229882727136 G Loss: -75.23259212253811 Wasserstein D: -0.34991086946500766\n",
      "Epoch 357 D Loss: -0.06965657214184741 G Loss: -83.45535721145309 Wasserstein D: -0.24488681179660182\n",
      "Epoch 358 D Loss: -0.1688434760887306 G Loss: -98.13588666582442 Wasserstein D: -0.2900600833492679\n",
      "Epoch 359 D Loss: -0.36691452239776823 G Loss: -93.1942596969071 Wasserstein D: -0.39908253062855115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:18<00:00,  4.52s/it]\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 360 D Loss: -0.42057266768875656 G Loss: -79.35086934549825 Wasserstein D: -0.4356536865234375\n",
      "Epoch 361 D Loss: -0.38054398223236724 G Loss: -78.92455014315519 Wasserstein D: -0.4169848248675153\n",
      "Epoch 362 D Loss: -0.38404600770323427 G Loss: -79.11497150767933 Wasserstein D: -0.43886053978980005\n",
      "Epoch 363 D Loss: -0.4328001329115221 G Loss: -78.46261863441734 Wasserstein D: -0.4687803575208971\n",
      "Epoch 364 D Loss: 0.20312860128762839 G Loss: -91.392532241928 Wasserstein D: -0.1928676258433949\n",
      "Epoch 365 D Loss: -0.23909482088955966 G Loss: -108.27182999190751 Wasserstein D: -0.2912818001700448\n",
      "Epoch 366 D Loss: -0.21750558172906195 G Loss: -107.85188869663052 Wasserstein D: -0.2324931004664281\n",
      "Epoch 367 D Loss: -0.3813788940856507 G Loss: -97.82782462593559 Wasserstein D: -0.4367014744898656\n",
      "Epoch 368 D Loss: -0.40802170013214323 G Loss: -84.87214100444234 Wasserstein D: -0.4463877111048132\n",
      "Epoch 369 D Loss: -0.27910403271655104 G Loss: -83.0768744728782 Wasserstein D: -0.43227146388767484\n",
      "Epoch 370 D Loss: -0.39787193778511526 G Loss: -83.59032488202715 Wasserstein D: -0.4627309412389368\n",
      "Epoch 371 D Loss: -0.23150624095143138 G Loss: -100.89059955090076 Wasserstein D: -0.30115551715130573\n",
      "Epoch 372 D Loss: -0.3021435237431026 G Loss: -89.49997775204533 Wasserstein D: -0.45141820307378167\n",
      "Epoch 373 D Loss: -0.38207762391417177 G Loss: -83.41019845175576 Wasserstein D: -0.47927482978447333\n",
      "Epoch 374 D Loss: -0.12123894858193564 G Loss: -108.91136831003469 Wasserstein D: -0.23239541220498253\n",
      "Epoch 375 D Loss: -0.31324311903306656 G Loss: -101.09259513374809 Wasserstein D: -0.41185771168528734\n",
      "Epoch 376 D Loss: 0.026129982688210228 G Loss: -89.16885237260298 Wasserstein D: -0.4092504327947443\n",
      "Epoch 377 D Loss: -0.39605782248757104 G Loss: -92.91537070107627 Wasserstein D: -0.4302131279365166\n",
      "Epoch 378 D Loss: -0.2867122196651005 G Loss: -94.13757340224473 Wasserstein D: -0.3606156035736724\n",
      "Epoch 379 D Loss: -0.34935397701663573 G Loss: -92.47781286706457 Wasserstein D: -0.45430483517946896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:18<00:00,  4.57s/it]\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 380 D Loss: -0.3990769553017783 G Loss: -94.26345617120916 Wasserstein D: -0.4215572864025623\n",
      "Epoch 381 D Loss: -0.4337920609054032 G Loss: -91.47387818023041 Wasserstein D: -0.45675813901674495\n",
      "Epoch 382 D Loss: -0.4983804342630026 G Loss: -90.18259995467179 Wasserstein D: -0.5129473492815778\n",
      "Epoch 383 D Loss: -0.3042022598373306 G Loss: -89.82871374383673 Wasserstein D: -0.5139758236758358\n",
      "Epoch 384 D Loss: -0.5337821586982353 G Loss: -90.24954362349077 Wasserstein D: -0.5458722414670291\n",
      "Epoch 385 D Loss: 2.791188913625437 G Loss: -127.75915180553089 Wasserstein D: -0.16920001690204328\n",
      "Epoch 386 D Loss: 0.47329978676109047 G Loss: -130.19839376169486 Wasserstein D: -0.2003907957277098\n",
      "Epoch 387 D Loss: -0.14673726542012674 G Loss: -129.13623409671382 Wasserstein D: -0.2683507745916193\n",
      "Epoch 388 D Loss: -0.26614523934317635 G Loss: -123.85174411160129 Wasserstein D: -0.3360420707222465\n",
      "Epoch 389 D Loss: -0.3650242198597301 G Loss: -119.15073432122077 Wasserstein D: -0.43208152931053323\n",
      "Epoch 390 D Loss: -0.3174325369454764 G Loss: -112.42743330735426 Wasserstein D: -0.451777744960118\n",
      "Epoch 391 D Loss: -0.39450265310861016 G Loss: -109.73537514426492 Wasserstein D: -0.45487944062773167\n",
      "Epoch 392 D Loss: -0.4347058809720553 G Loss: -109.99077526172557 Wasserstein D: -0.4583158693113527\n",
      "Epoch 393 D Loss: -0.3975242401336457 G Loss: -111.19256837217958 Wasserstein D: -0.45276295055042615\n",
      "Epoch 394 D Loss: -0.4070496459107299 G Loss: -115.18483990222424 Wasserstein D: -0.447702527879835\n",
      "Epoch 395 D Loss: -0.10477234100128387 G Loss: -113.75985301624645 Wasserstein D: -0.2945338962795018\n",
      "Epoch 396 D Loss: -0.37487982369803047 G Loss: -129.28038243647222 Wasserstein D: -0.3806689068987653\n",
      "Epoch 397 D Loss: -0.518268131709599 G Loss: -121.24168694769585 Wasserstein D: -0.5281853309044471\n",
      "Epoch 398 D Loss: -0.5071050470525568 G Loss: -111.41728567910361 Wasserstein D: -0.5469963233787697\n",
      "Epoch 399 D Loss: -0.5243741548978366 G Loss: -110.73251604200243 Wasserstein D: -0.5538290063818018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:18<00:00,  4.56s/it]\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Julius\\Documents\\LuH\\rna_sequencing\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400 D Loss: -0.21808871022471182 G Loss: -137.60755018754438 Wasserstein D: -0.3529713237202251\n",
      "Epoch 401 D Loss: -0.5124682979983883 G Loss: -128.6953130868765 Wasserstein D: -0.542417352849787\n",
      "Epoch 402 D Loss: -0.5168659770405376 G Loss: -114.25045429576527 Wasserstein D: -0.557783913779092\n",
      "Epoch 403 D Loss: -0.5751018390788899 G Loss: -112.94977009379781 Wasserstein D: -0.583795240708998\n",
      "Epoch 404 D Loss: -0.5765340311543925 G Loss: -114.03782611126667 Wasserstein D: -0.5913745906803157\n",
      "Epoch 405 D Loss: -0.3286699415086866 G Loss: -127.01954864288544 Wasserstein D: -0.47909791319520323\n",
      "Epoch 406 D Loss: -0.5600785475510818 G Loss: -119.43831565163352 Wasserstein D: -0.5810081641990822\n",
      "Epoch 407 D Loss: 0.14971971845293378 G Loss: -115.13709056294047 Wasserstein D: -0.39410197651469625\n",
      "Epoch 408 D Loss: -0.509437507682747 G Loss: -120.6946178516308 Wasserstein D: -0.5563265927188046\n",
      "Epoch 409 D Loss: -0.4120310003107244 G Loss: -117.30743472226017 Wasserstein D: -0.599032475398137\n",
      "Epoch 410 D Loss: -0.5824898139580147 G Loss: -116.21490062366833 Wasserstein D: -0.6127297994973776\n",
      "Epoch 411 D Loss: 0.5392508873572717 G Loss: -130.44357619919143 Wasserstein D: -0.3299051564890188\n",
      "Epoch 412 D Loss: -0.15483197298916904 G Loss: -148.93959632286658 Wasserstein D: -0.4549270309768357\n",
      "Epoch 413 D Loss: -0.579525874211238 G Loss: -122.00614539726631 Wasserstein D: -0.6017523812247323\n",
      "Epoch 414 D Loss: -0.46318923843490495 G Loss: -121.67948305356752 Wasserstein D: -0.5981722211504316\n",
      "Epoch 415 D Loss: -0.6151645900486232 G Loss: -121.16468469579736 Wasserstein D: -0.6296981064589707\n",
      "Epoch 416 D Loss: -0.5958770005019395 G Loss: -120.19282835847014 Wasserstein D: -0.6371182528409091\n",
      "Epoch 417 D Loss: -0.050382387388002624 G Loss: -130.65610402780814 Wasserstein D: -0.44175773567253057\n",
      "Epoch 418 D Loss: -0.278362727665401 G Loss: -134.74817380038175 Wasserstein D: -0.514435668091674\n",
      "Epoch 419 D Loss: -0.4565879981834572 G Loss: -129.54436903733475 Wasserstein D: -0.5856126905321242\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m\n\u001b[0;32m     10\u001b[0m run[neptune_logger\u001b[38;5;241m.\u001b[39mbase_namespace][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG_Structure\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(G)\n\u001b[0;32m     11\u001b[0m run[neptune_logger\u001b[38;5;241m.\u001b[39mbase_namespace][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD_Structure\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(D)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatentSpaceSize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlambda_gp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcritic_iter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneptune_logger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneptune_logger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m neptune_logger\u001b[38;5;241m.\u001b[39mlog_model()\n\u001b[0;32m     18\u001b[0m run\u001b[38;5;241m.\u001b[39mstop()\n",
      "Cell \u001b[1;32mIn[8], line 52\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(D, G, d_optimizer, g_optimizer, train_loader, val_loader, epochs, latentSpaceSize, lambda_gp, iters_critic, device, neptune_logger, run, trial)\u001b[0m\n\u001b[0;32m     49\u001b[0m NB_GENES \u001b[38;5;241m=\u001b[39m fake_data\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Perform random augmentations for stability\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m augmentations \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinomial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m fake_data \u001b[38;5;241m=\u001b[39m fake_data \u001b[38;5;241m+\u001b[39m augmentations[:, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, size\u001b[38;5;241m=\u001b[39m(NB_GENES,), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     54\u001b[0m real_data \u001b[38;5;241m=\u001b[39m real_data \u001b[38;5;241m+\u001b[39m augmentations[:,\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, size\u001b[38;5;241m=\u001b[39m(b_size,NB_GENES), device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run = neptune.init_run(\n",
    "    api_token=os.getenv(\"NEPTUNE_API_KEY\"),\n",
    "    project=os.getenv(\"NEPTUNE_PROJECT_NAME\"),\n",
    "    name=\"WGAN-GP - 800 - simple\",\n",
    ")\n",
    "\n",
    "neptune_logger = NeptuneLogger(run=run, model=G)\n",
    "                               \n",
    "run[neptune_logger.base_namespace][\"hyperparams\"] = stringify_unsupported(params)\n",
    "run[neptune_logger.base_namespace][\"G_Structure\"] = str(G)\n",
    "run[neptune_logger.base_namespace][\"D_Structure\"] = str(D)\n",
    "\n",
    "\n",
    "\n",
    "training(D, G, d_optimizer, g_optimizer, train_loader , val_loader, params[\"epochs\"], latentSpaceSize, params[\"lambda_gp\"], params[\"critic_iter\"], device, neptune_logger=neptune_logger, run=run)\n",
    "\n",
    "neptune_logger.log_model()\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "#torch.save(G.state_dict(), f\"{git_root}/experiments/generating/models/G_800_WGAN_GP.pt\")\n",
    "#torch.save(D.state_dict(), f\"{git_root}/experiments/generating/models/D_800_WGAN_GP.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.1)\n",
       "    (3): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.1)\n",
       "    (6): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "    (7): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): LeakyReLU(negative_slope=0.1)\n",
       "    (9): Linear(in_features=2048, out_features=5045, bias=True)\n",
       "  )\n",
       "  (embedding): Embedding(105, 2)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1270,  0.0696, -0.2601,  ..., -1.1326, -0.1175,  0.0132]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G(torch.randn(1, latentSpaceSize, device=device), y_Val[0].long().unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000,  ..., 3.3789, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(22.0881)\n"
     ]
    }
   ],
   "source": [
    "# Classify the discriminator on the first 50 validation data points\n",
    "with torch.no_grad():\n",
    "    D.eval().cpu()\n",
    "    y_pred = D(X_Val[:50], y_Val[:50].long())\n",
    "    print(y_pred.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.1170e-01, -7.4901e-01,  2.0443e-01,  ...,  2.1142e+00,\n",
      "          5.0957e-01,  7.0062e-01],\n",
      "        [ 8.7831e-03,  1.4698e-01, -9.8590e-02,  ..., -1.6326e+00,\n",
      "         -1.4369e-02, -3.0847e-01],\n",
      "        [ 3.7625e-01, -1.4402e-01, -2.8685e-01,  ...,  2.5300e+00,\n",
      "         -4.7173e-01, -3.8767e-01],\n",
      "        ...,\n",
      "        [-9.4485e-02,  3.4533e-01,  3.4342e-01,  ..., -1.6533e+00,\n",
      "          1.4203e-01, -2.1923e-01],\n",
      "        [ 4.0288e-01,  9.9806e-02,  2.4194e-01,  ..., -2.2537e-01,\n",
      "         -3.7465e-02,  1.4169e-03],\n",
      "        [ 8.6841e-01, -2.4582e-01,  4.3270e-02,  ...,  1.8616e+00,\n",
      "          1.0579e-01, -5.7536e-02]])\n",
      "tensor(240.7790)\n"
     ]
    }
   ],
   "source": [
    "# Generate 50 samples\n",
    "with torch.no_grad():\n",
    "    G.eval().cpu()\n",
    "    fake_data = G(torch.randn(50, latentSpaceSize), y_Val[:50].long())\n",
    "    print(fake_data)\n",
    "\n",
    "# Evaluate the generated data on the discriminator\n",
    "with torch.no_grad():\n",
    "    D.eval().cpu()\n",
    "    y_pred = D(fake_data, y_Val[:50].long())\n",
    "    print(y_pred.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discriminator classifies the generated output clearly as fake. Fake is positiv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
